{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import outer, eye, ones, zeros, diag, log, sqrt, exp, pi\n",
    "from numpy.linalg import inv, solve\n",
    "from numpy.random import multivariate_normal as mvnormal, normal, gamma, beta, binomial\n",
    "from scipy.special import gammaln\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy.random import randn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, min, max, sqrt, mean, std\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise') # TODO REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "mm_cumulative_error = 0.0 # TODO REMOVE\n",
    "mm_max_diff = 0.0 # TODO REMOVE\n",
    "\n",
    "class EM_algo():\n",
    "    \"\"\"\n",
    "        A superclass for different EM-fitted models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparams, X=None, Y=None, ndata=0, pdata=0):\n",
    "        \"\"\"\n",
    "            Initialize model based either on given data (X, Y) or\n",
    "            on given data dimensionality (ndata, pdata).\n",
    "        \"\"\"\n",
    "        if not X is None and not Y is None:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "            self.ndata = len(self.X)\n",
    "            self.pdata = len(self.X[0])\n",
    "        if ndata and pdata:\n",
    "            self.X = None\n",
    "            self.Y = None\n",
    "            self.ndata = ndata\n",
    "            self.pdata = pdata\n",
    "        self.h = hyperparams\n",
    "        self.p = dict() # model parameters\n",
    "        self.reset()\n",
    "        if not X is None and not Y is None:\n",
    "            self.current_logl, self.cll = self.logl()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the drawn y and z-values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_fit(self, alim=1e-10, maxit=1e4):\n",
    "        \"\"\"\n",
    "            Calls the EM_iter repeatedly until the log likelihood\n",
    "            of the model increases less than 'alim' in absolute\n",
    "            value or after 'maxit' iterations have been done.\n",
    "\n",
    "            Returns the number of EM-iterations, final log likelihood\n",
    "            value and a string that explains the end condition.\n",
    "        \"\"\"\n",
    "        logl, ll = self.logl()\n",
    "        for i in range(int(maxit)):\n",
    "            self.EM_iter()\n",
    "            logl2, ll2 = self.logl()\n",
    "            adiff = abs(logl2 - logl)\n",
    "            if adiff < alim:\n",
    "                return i+1, logl2, \"alim\"\n",
    "            logl = logl2\n",
    "        return maxit, logl2, \"maxit\"\n",
    "\n",
    "\n",
    "    def assert_logl_increased(self, event):\n",
    "        \"\"\"\n",
    "            Checks that the log likelihood increased since model\n",
    "            initialization or the time this function was last called.\n",
    "        \"\"\"\n",
    "        newlogl, ll = self.logl()\n",
    "        global mm_cumulative_error # TODO REMOVE\n",
    "        global mm_max_diff # TODO REMOVE\n",
    "        if self.current_logl > newlogl:\n",
    "            if mm_max_diff < self.current_logl - newlogl:\n",
    "                mm_max_diff = self.current_logl - newlogl\n",
    "            mm_cumulative_error = mm_cumulative_error + (self.current_logl - newlogl)\n",
    "        if self.current_logl - newlogl > 0.5:\n",
    "            print(\"Likelihood decreased by: \" + str(self.current_logl - newlogl))\n",
    "#         if self.current_logl - newlogl > 1e-3:\n",
    "            raise ValueError(\"logl decreased after %s\" % (event))\n",
    "        self.current_logl, self.cll = newlogl, ll\n",
    "\n",
    "\n",
    "    def get_p(self):\n",
    "        \"\"\"\n",
    "            Returns a copy of the model parameters.\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.p)\n",
    "\n",
    "\n",
    "    def set_p(self, p):\n",
    "        \"\"\"\n",
    "            Sets the model parameters.\n",
    "        \"\"\"\n",
    "        self.p = p.copy()\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        for k, v in self.p.items():\n",
    "            print(\"%s = %s\" % (k, v))\n",
    "\n",
    "\n",
    "    def pretty_vector(self, x):\n",
    "        \"\"\"\n",
    "            Returns a formatted version of a vector.\n",
    "        \"\"\"\n",
    "        s = [\"(\"]\n",
    "        s.extend([\"%.2f, \" % (xi) for xi in x[:-1]])\n",
    "        s.append(\"%.2f)\" % (x[-1]))\n",
    "        return \"\".join(s)\n",
    "\n",
    "\n",
    "    def debug_logl(self, ll1, ll2):\n",
    "        \"\"\"\n",
    "            Prints an analysis of the per-term change in\n",
    "            log likelihood from ll1 to ll2.\n",
    "        \"\"\"\n",
    "        print(\"Logl      before     after\")\n",
    "        for v1, v2, i in zip(ll1, ll2, range(len(ll1))):\n",
    "            if v1 > v2:\n",
    "                d = \">\"\n",
    "            elif v2 > v1:\n",
    "                d = \"<\"\n",
    "            else:\n",
    "                d = \"=\"\n",
    "            print(\"Term %02d: %7.3f %s %7.3f\" % (i, v1, d, v2))\n",
    "        v1 = sum(ll1)\n",
    "        v2 = sum(ll2)\n",
    "        if v1 > v2:\n",
    "            d = \">\"\n",
    "        elif v2 > v1:\n",
    "            d = \"<\"\n",
    "        else:\n",
    "            d = \"=\"\n",
    "        diff = v2-v1\n",
    "        print(\"Total    %7.3f %s %7.3f   diff: %7.3f\" % (v1, d, v2, diff))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "class EM_algo_LM(EM_algo):\n",
    "    \"\"\"\n",
    "        A linear gaussian model.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "\n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p           = dict()\n",
    "        self.p[\"sigma2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value (and a constant z-value for compatibility)\n",
    "        \"\"\"\n",
    "        mean = float(item.dot(self.p[\"phi\"]))\n",
    "        std  = sqrt(self.p[\"sigma2\"])\n",
    "        return normal(mean, std), 1\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll    = zeros(8)\n",
    "        phie  = self.p[\"phi\"] - self.mu_phi0\n",
    "        err   = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        # p(y)\n",
    "        ll[0] = - 0.5 * log(2 * pi * self.p[\"sigma2\"]) * self.ndata\n",
    "        ll[1] = sum(- 0.5 * err / self.p[\"sigma2\"])\n",
    "        # p(phi)\n",
    "        ll[2] = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2\"]) * self.pdata\n",
    "        ll[3] = - 0.5 * phie.T.dot(phie) / (self.lbd_phi0 * self.p[\"sigma2\"])\n",
    "        # p(sigma2)\n",
    "        ll[4] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[5] = - gammaln(self.alpha_s20)\n",
    "        ll[6] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2\"])\n",
    "        ll[7] = - self.beta_s20 / self.p[\"sigma2\"]\n",
    "        return sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        # phi\n",
    "        sumxx         = self.X.T.dot(self.X)\n",
    "        sumxy         = self.X.T.dot(self.Y)\n",
    "        sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "        self.p[\"phi\"] = solve(sigma_phi_inv, sigma_mu + sumxy)\n",
    "        self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "        # sigma2\n",
    "        phie = (self.p[\"phi\"] - self.mu_phi0) ** 2\n",
    "        err  = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        num  = self.beta_s20 + 0.5 * sum(err) + 0.5 * sum(phie) / self.lbd_phi0\n",
    "        den  = self.alpha_s20 + 1.0 + 0.5 * (self.ndata + self.pdata)\n",
    "        self.p[\"sigma2\"] = num / den\n",
    "        if self.p[\"sigma2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2 < 0.0\")\n",
    "        self.assert_logl_increased(\"sigma2 update\")\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        print(\"phi    : %s\" % (self.pretty_vector(self.p[\"phi\"])))\n",
    "        print(\"sigma2 : %.3f\" % (self.p[\"sigma2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "def generate_X(ndata, pdata):\n",
    "    \"\"\"\n",
    "        Return a matrix of normally distributed random values.\n",
    "    \"\"\"\n",
    "    X = randn(ndata, pdata)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_YZ(X, distribution):\n",
    "    \"\"\"\n",
    "        Draw observations Y and latent variable values Z from a distribution.\n",
    "    \"\"\"\n",
    "    ndata = len(X)\n",
    "    Y = zeros(ndata)\n",
    "    Z = zeros(ndata)\n",
    "    for i in range(ndata):\n",
    "        Y[i], Z[i] = distribution.draw(X[i])\n",
    "    return Y, Z\n",
    "\n",
    "\n",
    "def get_hyperp():\n",
    "    \"\"\"\n",
    "        Return model hyperparameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "            \"alpha_s20\": 5.0,\n",
    "            \"beta_s20\" : 1.0,\n",
    "            \"lbd_phi0\" : 1.0,\n",
    "            \"mu_phi0\"  : 0.0,\n",
    "            \"alpha_w0\" : 3.0,\n",
    "            \"beta_w0\"  : 3.0,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EM_algo_MM(EM_algo):\n",
    "    \"\"\"\n",
    "        A mixture of two linear models.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.alpha_w0       = self.h[\"alpha_w0\"]\n",
    "        self.beta_w0        = self.h[\"beta_w0\"]\n",
    "\n",
    "        # Same priors for phi1 and phi2, s2_1, s2_2, don't bother to copy vars twice\n",
    "        # i.e. alpha_s2_1_0 = alpha_s2_2_0 = alpha_s20\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "        \n",
    "        # Precalculations:\n",
    "        self.w_gamma_ln_multiplier  = gammaln(self.alpha_w0 + self.beta_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.alpha_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.beta_w0)\n",
    "        \n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p             = dict()\n",
    "        # Weights\n",
    "        self.p[\"w\"]        = beta(self.alpha_w0, self.beta_w0)\n",
    "        # Responsibilities\n",
    "        self.gamma         = binomial(1, self.p[\"w\"], self.ndata)\n",
    "        # Component 1\n",
    "        self.p[\"sigma2_1\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_1\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_1\"] * self.sigma_phi0)\n",
    "        # Component 2\n",
    "        self.p[\"sigma2_2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_2\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value and z-value\n",
    "        \"\"\"\n",
    "        mean1 = float(item.dot(self.p[\"phi_1\"]))\n",
    "        std1  = sqrt(self.p[\"sigma2_1\"])\n",
    "        mean2 = float(item.dot(self.p[\"phi_2\"]))\n",
    "        std2  = sqrt(self.p[\"sigma2_2\"])\n",
    "        \n",
    "        if np.random.rand() < self.p[\"w\"]:\n",
    "            return normal(mean1, std1), 1\n",
    "        else:\n",
    "            return normal(mean2, std2), 0\n",
    "\n",
    "\n",
    "    def logl(self, really=False):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        \n",
    "        if not really:\n",
    "            return self.incompletelogl()\n",
    "        \n",
    "        ll         = zeros(20)\n",
    "        phi_1_diff = self.p[\"phi_1\"] - self.mu_phi0\n",
    "        phi_2_diff = self.p[\"phi_2\"] - self.mu_phi0\n",
    "        phi_1_err  = phi_1_diff.T.dot(phi_1_diff)\n",
    "        phi_2_err  = phi_2_diff.T.dot(phi_2_diff)\n",
    "        err_1      = (self.Y - self.X.dot(self.p[\"phi_1\"])) ** 2\n",
    "        err_2      = (self.Y - self.X.dot(self.p[\"phi_2\"])) ** 2\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        \n",
    "        ### posterior factorizes p(y,z,w,phi,sigma) = p(y,z)p(w)p(phi)p(sigma)\n",
    "        #                                           = p(y)p(z)p(w)p(phi)p(sigma)\n",
    "        \n",
    "        ### p(y,z)\n",
    "        ll[0] =     gamma.dot(    self.p[\"w\"]  * norm.logpdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"])) )\n",
    "        ll[1] = (1-gamma).dot( (1-self.p[\"w\"]) * norm.logpdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"])) )\n",
    "        \n",
    "        ### p(z) already in p(y,z)\n",
    "#         ll[4] = np.sum((gamma * log(self.p[\"w\"])) + ((1 - gamma) * log(1 - self.p[\"w\"])))\n",
    "        \n",
    "        ### p(w)\n",
    "        ll[5] = self.w_gamma_ln_multiplier\n",
    "        ll[6] = (self.alpha_w0 - 1) * self.p[\"w\"]\n",
    "        ll[7] = (self.beta_w0  - 1) * (1 - self.p[\"w\"])\n",
    "\n",
    "        ### p(phi)\n",
    "        # phi_1\n",
    "        ll[8]  = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_1\"]) + log(self.lbd_phi0) )\n",
    "        ll[9]  = - 0.5 * phi_1_err / (self.lbd_phi0 * self.p[\"sigma2_1\"])\n",
    "        # phi_2\n",
    "        ll[10] = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_2\"]) + log(self.lbd_phi0) )\n",
    "        ll[11] = - 0.5 * phi_2_err / (self.lbd_phi0 * self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(sigma2)\n",
    "        # sigma2_1\n",
    "        ll[12] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[13] = - gammaln(self.alpha_s20)\n",
    "        ll[14] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_1\"])\n",
    "        ll[15] = - self.beta_s20 / self.p[\"sigma2_1\"]\n",
    "        # sigma2_2\n",
    "        ll[16] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[17] = - gammaln(self.alpha_s20)\n",
    "        ll[18] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_2\"])\n",
    "        ll[19] = - self.beta_s20 / self.p[\"sigma2_2\"]\n",
    "        \n",
    "        return np.sum(ll), ll\n",
    "\n",
    "\n",
    "    def incompletelogl(self):\n",
    "        \"\"\"\n",
    "            Calculates the incomplete data log likelihood for this model.\n",
    "            Returns the incomplete logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll         = zeros(20)\n",
    "        phi_1_diff = self.p[\"phi_1\"] - self.mu_phi0\n",
    "        phi_2_diff = self.p[\"phi_2\"] - self.mu_phi0\n",
    "        phi_1_err  = phi_1_diff.T.dot(phi_1_diff)\n",
    "        phi_2_err  = phi_2_diff.T.dot(phi_2_diff)\n",
    "        \n",
    "        ### p(y)\n",
    "        N1 = norm.pdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"]))\n",
    "        N2 = norm.pdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"]))\n",
    "        ll[0] = np.sum( np.log( self.p[\"w\"]*N1 + (1-self.p[\"w\"])*N2 ) )\n",
    "        \n",
    "        ### p(w)\n",
    "        ll[1] = self.w_gamma_ln_multiplier\n",
    "        ll[2] = (self.alpha_w0 - 1) * self.p[\"w\"]\n",
    "        ll[3] = (self.beta_w0  - 1) * (1 - self.p[\"w\"])\n",
    "\n",
    "        ### p(phi)\n",
    "        # phi_1\n",
    "        ll[4]  = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_1\"]) + log(self.lbd_phi0) )\n",
    "        ll[5]  = - 0.5 * phi_1_err / (self.lbd_phi0 * self.p[\"sigma2_1\"])\n",
    "        # phi_2\n",
    "        ll[6] = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_2\"]) + log(self.lbd_phi0) )\n",
    "        ll[7] = - 0.5 * phi_2_err / (self.lbd_phi0 * self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(sigma2)\n",
    "        # sigma2_1\n",
    "        ll[8] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[9] = - gammaln(self.alpha_s20)\n",
    "        ll[10] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_1\"])\n",
    "        ll[11] = - self.beta_s20 / self.p[\"sigma2_1\"]\n",
    "        # sigma2_2\n",
    "        ll[12] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[13] = - gammaln(self.alpha_s20)\n",
    "        ll[14] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_2\"])\n",
    "        ll[15] = - self.beta_s20 / self.p[\"sigma2_2\"]\n",
    "        \n",
    "        return np.sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ==================== E-STEP ====================\n",
    "      \n",
    "        # norm.pdf works on a vector, returning probability for each separately\n",
    "        propto_gamma1 =      self.p[\"w\"]  * norm.pdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"]))\n",
    "        propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"]))\n",
    "\n",
    "        self.gamma = propto_gamma1 / (propto_gamma1 + propto_gamma2) # responsibilities\n",
    "\n",
    "        # ==================== M-STEP ====================\n",
    "\n",
    "        # ========== Component weights w ==========\n",
    "        num = 2*np.sum(self.gamma) + self.alpha_w0 - 1\n",
    "        den = 2*self.ndata + self.alpha_w0 + self.beta_w0 - 2\n",
    "        self.p[\"w\"] = num / den\n",
    "\n",
    "        self.assert_logl_increased(\"w\")\n",
    "    \n",
    "    \n",
    "        # ========== Variances sigma2 ==========\n",
    "        # phi_1 and phi_2 still have the previous value, i.e. from step s, we are calculating sigma for step s+1\n",
    "        \n",
    "        # sigma2_1\n",
    "        phie = np.sum((self.p[\"phi_1\"] - self.mu_phi0) ** 2)  / self.lbd_phi0\n",
    "        phiX = self.p[\"phi_1\"].dot(self.X.T)\n",
    "        target_err = (self.Y - phiX)**2\n",
    "        err = self.gamma.dot(target_err)\n",
    "        num = 2*self.beta_s20 + err + phie\n",
    "        den = 2*self.alpha_s20 + 2.0 + np.sum(self.gamma) + self.pdata\n",
    "        self.p[\"sigma2_1\"] = num / den\n",
    "        if self.p[\"sigma2_1\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_1 < 0.0\")\n",
    "        \n",
    "        # sigma2_2\n",
    "        phie = np.sum((self.p[\"phi_2\"] - self.mu_phi0) ** 2)  / self.lbd_phi0\n",
    "        phiX = self.p[\"phi_2\"].dot(self.X.T)\n",
    "        target_err = (self.Y - phiX)**2\n",
    "        err = (1-self.gamma).dot(target_err)\n",
    "        num = 2*self.beta_s20 + err + phie\n",
    "        den = 2*self.alpha_s20 + 2.0 + np.sum(1-self.gamma) + self.pdata\n",
    "        self.p[\"sigma2_2\"] = num / den\n",
    "        if self.p[\"sigma2_2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_2 < 0.0\")\n",
    "\n",
    "#         self.assert_logl_increased(\"sigma2 update\")\n",
    "        \n",
    "        \n",
    "        # ========== Variables phi ==========\n",
    "        \n",
    "        # phi_1\n",
    "        sum_gammayx = self.gamma.T.dot( (Y * self.X.T).T )\n",
    "        resp_matrix = eye(self.ndata) * self.gamma\n",
    "        sum_gammaxx = self.X.T.dot(resp_matrix.dot(self.X))\n",
    "        sigma_mu        = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv   = self.sigma_phi0_inv + sum_gammaxx\n",
    "        self.p[\"phi_1\"] = solve(sigma_phi_inv, sigma_mu + sum_gammayx)\n",
    "        \n",
    "        # phi_2\n",
    "        sum_gammayx = (1-self.gamma).T.dot(  (Y * self.X.T).T  )\n",
    "        resp_matrix = eye(self.ndata) * (1-self.gamma)\n",
    "        sum_gammaxx = self.X.T.dot(resp_matrix.dot(self.X))\n",
    "        sigma_mu        = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv   = self.sigma_phi0_inv + sum_gammaxx\n",
    "        self.p[\"phi_2\"] = solve(sigma_phi_inv, sigma_mu + sum_gammayx)\n",
    "\n",
    "        self.assert_logl_increased(\"phi update\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 training data and 50 validation data from true model:\n",
      "phi_2 = [ 0.06157095 -0.04221531  0.31535768]\n",
      "sigma2_1 = 0.14622875187497333\n",
      "phi_1 = [ 0.1648381  -0.42739616  0.68170409]\n",
      "w = 0.5654753035666211\n",
      "sigma2_2 = 0.2009333607240098\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get hyperparameters for model\n",
    "hyperp = get_hyperp()\n",
    "# generate 50 training data and 20 validation data locations of dim=1\n",
    "ndata = 50\n",
    "ndata_v = 50\n",
    "pdata = 3\n",
    "X = generate_X(ndata, pdata)\n",
    "X_v = generate_X(ndata_v, pdata)\n",
    "\n",
    "true_model = EM_algo_MM(hyperp, ndata=ndata, pdata=pdata)\n",
    "Y, Z = generate_YZ(X, true_model)\n",
    "Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "    (ndata, ndata_v))\n",
    "true_model.print_p()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# generate a model for estimating the parameters of the\n",
    "# true model based on the observations (X, Y) we just made\n",
    "# model = EM_algo_MM(hyperp, X, Y)\n",
    "model = EM_algo_MM(hyperp, X, Y)\n",
    "i, logl, r = model.EM_fit()\n",
    "print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "        (logl, i, r))\n",
    "print(\"\")\n",
    "print(\"MAP estimate of true model parameters:\")\n",
    "model.print_p()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if possible, plot samples, true model and estimated model\n",
    "if pdata == 1:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "#         plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "#         print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "#                 true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "#         print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "#                 model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "\n",
    "    y = true_model.p[\"phi_1\"] * x\n",
    "    color = 'orange'\n",
    "    plt.plot(x, y, color, label=\"true1\")\n",
    "#     plt.fill_between(x, y + 1.96 * sqrt(true_model.p[\"sigma2_1\"]), y - 1.96 * sqrt(true_model.p[\"sigma2_1\"]), alpha=0.1, facecolor=color, interpolate=True)\n",
    "    \n",
    "    y = true_model.p[\"phi_2\"] * x\n",
    "    color = 'green'\n",
    "    plt.plot(x, y, color, label=\"true1\")\n",
    "#     plt.fill_between(x, y + 1.96 * sqrt(true_model.p[\"sigma2_2\"]), y - 1.96 * sqrt(true_model.p[\"sigma2_2\"]), alpha=0.1, facecolor=color, interpolate=True)\n",
    "\n",
    "    # Components\n",
    "    y = model.p[\"phi_1\"] * x\n",
    "    color = 'red'\n",
    "    plt.plot(x, y, color, label=\"component1\")\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(model.p[\"sigma2_1\"]), y - 1.96 * sqrt(model.p[\"sigma2_1\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "    y = model.p[\"phi_2\"] * x\n",
    "    color = 'blue'\n",
    "    plt.plot(x, y, color, label=\"component2\")\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(model.p[\"sigma2_2\"]), y - 1.96 * sqrt(model.p[\"sigma2_2\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mm_cumulative_error = 0.0\n",
    "mm_max_diff = 0.0\n",
    "foodict = {'w': 0, 'phi_1': 0, 'phi_2': 0, 'sigma2_1': 0, 'sigma2_2': 0}\n",
    "bardict = {'w': 0, 'phi_1': 0, 'phi_2': 0, 'sigma2_1': 0, 'sigma2_2': 0}\n",
    "counter = 0\n",
    "counterOther = 0\n",
    "counterAlim = 0\n",
    "for truru in range(50):\n",
    "    try:\n",
    "        # generate a model for estimating the parameters of the\n",
    "        # true model based on the observations (X, Y) we just made\n",
    "        model = EM_algo_MM(hyperp, X, Y)\n",
    "        i, logl, r = model.EM_fit()\n",
    "        if r == \"alim\":\n",
    "            counterAlim += 1\n",
    "    except ValueError as e:\n",
    "        counter = counter + 1\n",
    "        foodict[str(e)] = foodict[str(e)] + 1\n",
    "    except RuntimeError as e:\n",
    "        counterOther = counterOther + 1\n",
    "        bardict[str(e)] = bardict[str(e)] + 1\n",
    "\n",
    "print(foodict)\n",
    "print(counter, counterOther, counterAlim)\n",
    "print(bardict)\n",
    "print(mm_cumulative_error)\n",
    "print(mm_max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P=1 2*w check each\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 191 809\n",
    "# {'phi_2': 47, 'w': 0, 'phi_1': 39, 'sigma2_2': 104, 'sigma2_1': 1}\n",
    "# 97.4147580405\n",
    "# 3.59291931674\n",
    "# P=1 w check each\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 200 800\n",
    "# {'phi_2': 71, 'w': 4, 'phi_1': 32, 'sigma2_2': 93, 'sigma2_1': 0}\n",
    "# 92.1741867772\n",
    "# 2.57140877529\n",
    "# P=1 w check w and after\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 2 998\n",
    "# {'phi_2': 0, 'w': 2, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 12.8199038625\n",
    "# 0.191820591439\n",
    "# P=1 2*w check w and after\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 2.98402674426\n",
    "# 0.0398580346214\n",
    "# prev with update to phi logl calc formula...\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 2.84134098953\n",
    "# 0.0422773055005\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 5.68757981874\n",
    "# 0.0336705559807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P=25 2*w check?\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 3.08312064566\n",
    "# 0.00761159372594\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 3.37453377128\n",
    "# 0.00951989469854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        Executed when program is run.\n",
    "    \"\"\"\n",
    "    print(\"Starting program\")\n",
    "    print(\"\")\n",
    "    test_LM_model()\n",
    "\n",
    "\n",
    "def test_LM_model():\n",
    "    \"\"\"\n",
    "        Example that demonstrates how to call the model.\n",
    "    \"\"\"\n",
    "    # get hyperparameters for model\n",
    "    hyperp = get_hyperp()\n",
    "    # generate 50 training data and 20 validation data locations of dim=1\n",
    "    ndata = 20\n",
    "    ndata_v = 50\n",
    "    pdata = 3\n",
    "    X = generate_X(ndata, pdata)\n",
    "    X_v = generate_X(ndata_v, pdata)\n",
    "    # intialize true model randomly and draw observations from it\n",
    "    true_model = EM_algo_LM(hyperp, ndata=ndata, pdata=pdata)\n",
    "    Y, Z = generate_YZ(X, true_model)\n",
    "    Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "    print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "            (ndata, ndata_v))\n",
    "    true_model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # generate a model for estimating the parameters of the\n",
    "    # true model based on the observations (X, Y) we just made\n",
    "    model = EM_algo_LM(hyperp, X, Y)\n",
    "    i, logl, r = model.EM_fit()\n",
    "    print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "            (logl, i, r))\n",
    "    print(\"\")\n",
    "    print(\"MAP estimate of true model parameters:\")\n",
    "    model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # crossvalidate the estimated model with the validation data\n",
    "    fit_params = model.get_p()\n",
    "    model_v = EM_algo_LM(hyperp, X_v, Y_v)\n",
    "    model_v.set_p(fit_params)\n",
    "    logl, ll = model_v.logl()\n",
    "    print(\"Crossvalidated logl: %.2f\" % (logl))\n",
    "\n",
    "    # if possible, plot samples, true model and estimated model\n",
    "    if pdata != 1:\n",
    "        return\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "    plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "    print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "            true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "    print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "            model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def print_linear_model(x, phi, sigma2, color, label):\n",
    "    \"\"\"\n",
    "        Print linear model mean and 95% confidence interval.\n",
    "    \"\"\"\n",
    "    y = phi * x\n",
    "    plt.plot(x, y, color, label=label)\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(sigma2), y - 1.96 * sqrt(sigma2), \\\n",
    "            alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - Test mixture model\n",
    "\n",
    "Generate training and validation data from the mixture model. Analyze how well the model (trained with the training data) can explain the validation data with different data dimensionality and different amounts of generated data when you only do the fitting once. How do the results change, and why, when you start the EM from multiple locations and choose the best fit? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([16, 8, 4, 2, 1], [128, 64, 32, 16, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dimensions = []\n",
    "datasamples = []\n",
    "\n",
    "\n",
    "for exponent in range(0, 5):\n",
    "    dimensions.append(2 ** exponent)\n",
    "for exponent in range(3, 8):\n",
    "    datasamples.append(2 ** exponent)\n",
    "\n",
    "dimensions.reverse()\n",
    "datasamples.reverse()\n",
    "\n",
    "dimensions, datasamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over combinations of dimensions and dataset sizes and \n",
    "\n",
    "1. run a single EM with this setting \n",
    "2. run multiple EM's with the same setting\n",
    "3. compare / log the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "DONE! Multi inits used 100 initializations.\n",
      " \n",
      "P \tT\tSingle Train\tMulti Train\tδ Train\t\tSingle Test\tMulti Test\tδ Test\n",
      "16 & 128 & -239.79 & -150.54 & -89.25 & -118.51 & -77.28 & -41.23\\\\\n",
      "16 & 64 & -47.09 & -47.09 & -0.00 & -60.07 & -60.07 & 0.00\\\\\n",
      "16 & 32 & -28.29 & -22.46 & -5.84 & -48.46 & -48.81 & 0.35\\\\\n",
      "16 & 16 & -3.55 & -3.55 & -0.00 & -105.22 & -105.22 & 0.00\\\\\n",
      "16 & 8 & 15.53 & 15.53 & -0.00 & 12.41 & 12.41 & 0.00\\\\\n",
      "8 & 128 & -116.32 & -116.32 & -0.00 & -45.82 & -45.82 & 0.00\\\\\n",
      "8 & 64 & -27.83 & -27.83 & -0.00 & -17.98 & -17.98 & -0.00\\\\\n",
      "8 & 32 & -41.25 & -26.56 & -14.69 & -37.56 & -25.33 & -12.23\\\\\n",
      "8 & 16 & -5.51 & -5.01 & -0.51 & -6.24 & -6.14 & -0.10\\\\\n",
      "8 & 8 & -4.47 & -0.37 & -4.09 & -9.77 & -14.14 & 4.36\\\\\n",
      "4 & 128 & -78.04 & -78.04 & -0.00 & -15.99 & -15.99 & -0.00\\\\\n",
      "4 & 64 & -35.86 & -35.86 & -0.00 & -17.61 & -17.61 & 0.00\\\\\n",
      "4 & 32 & -19.28 & -19.28 & -0.00 & -8.96 & -8.96 & 0.00\\\\\n",
      "4 & 16 & -5.36 & -5.36 & -0.00 & 0.27 & 0.27 & 0.00\\\\\n",
      "4 & 8 & -6.85 & -6.85 & -0.00 & -3.41 & -3.41 & 0.00\\\\\n",
      "2 & 128 & -132.82 & -132.82 & -0.00 & -42.92 & -42.92 & -0.00\\\\\n",
      "2 & 64 & -21.59 & -21.59 & -0.00 & -5.95 & -5.95 & 0.00\\\\\n",
      "2 & 32 & -7.74 & -7.74 & -0.00 & 5.28 & 5.28 & -0.00\\\\\n",
      "2 & 16 & -5.18 & -5.18 & -0.00 & -1.05 & -1.05 & -0.00\\\\\n",
      "2 & 8 & 2.19 & 2.19 & -0.00 & 3.76 & 3.76 & -0.00\\\\\n",
      "1 & 128 & -117.97 & -117.97 & -0.00 & -49.94 & -49.94 & -0.00\\\\\n",
      "1 & 64 & -39.05 & -39.05 & -0.00 & -5.51 & -5.51 & 0.00\\\\\n",
      "1 & 32 & -15.64 & -15.64 & -0.00 & 0.45 & 0.45 & 0.00\\\\\n",
      "1 & 16 & -4.65 & -4.65 & -0.00 & 3.68 & 3.68 & -0.00\\\\\n",
      "1 & 8 & 6.03 & 6.03 & -0.00 & -1.35 & -1.35 & -0.00\\\\\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwestrup/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "validation_set_size = 0.3\n",
    "num_multi_inits = 100\n",
    "epoch = 0\n",
    "\n",
    "results_table = (\"P \\t\" + \"T\\t\" \n",
    "                 + \"Single Train\\t\" + \"Multi Train\\t\" + \"δ Train\\t\\t\" \n",
    "                 + \"Single Test\\t\" + \"Multi Test\\t\" + \"δ Test\" + \"\\n\")\n",
    "\n",
    "for P in dimensions:\n",
    "    for T in datasamples:\n",
    "        \n",
    "        ## INIT\n",
    "        \n",
    "        # get hyperparameters for model\n",
    "        hyperp = get_hyperp()\n",
    "        # generate data\n",
    "        ndata = T\n",
    "        ndata_v = np.ceil(T * 0.3)\n",
    "        pdata = P\n",
    "        X = generate_X(ndata, pdata)\n",
    "        X_v = generate_X(ndata_v, pdata)\n",
    "        # generate true model\n",
    "        true_model = EM_algo_MM(hyperp, ndata=ndata, pdata=pdata)\n",
    "        Y, Z = generate_YZ(X, true_model)\n",
    "        Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "        \n",
    "        ## Do a single run of EM:\n",
    "                \n",
    "        # train\n",
    "        model_train = EM_algo_MM(hyperp, X, Y)\n",
    "        i, logl_train_single, r = model_train.EM_fit()\n",
    "        # test / validate\n",
    "        fit_params = model_train.get_p()\n",
    "        model_test = EM_algo_MM(hyperp, X_v, Y_v)\n",
    "        model_test.set_p(fit_params)\n",
    "        logl_test_single, ll = model_test.logl()\n",
    "                    \n",
    "        ## Do multiple initializations of EM:\n",
    "        \n",
    "        # train\n",
    "        best_model = EM_algo_MM(hyperp, X, Y)\n",
    "        i, best_logl_train_multi, r = best_model.EM_fit()\n",
    "        for k in range(0, num_multi_inits):\n",
    "            \n",
    "            # print progress\n",
    "            epoch += 1\n",
    "            total = len(dimensions) * len(datasamples) * num_multi_inits\n",
    "            percent = epoch/total*100\n",
    "            print(\"%.2f\" % percent, end=\"\\r\")\n",
    "            \n",
    "            model_train = EM_algo_MM(hyperp, X, Y)\n",
    "            i, logl_train_multi, r = model_train.EM_fit()\n",
    "            if (logl_train_multi > best_logl_train_multi):\n",
    "                best_logl_train_multi = logl_train_multi\n",
    "                best_model = model_train\n",
    "                \n",
    "        # test / validate\n",
    "        fit_params = best_model.get_p()\n",
    "        model_test = EM_algo_MM(hyperp, X_v, Y_v)\n",
    "        model_test.set_p(fit_params)\n",
    "        logl_test_multi, ll = model_test.logl()\n",
    "        \n",
    "#         # nice print \n",
    "#         results_table += (str(P) + \"\\t\" + str(T) + \"\\t\" \n",
    "#                           + \"%.2f\" % logl_train_single + \"\\t\\t\"\n",
    "#                           + \"%.2f\" % best_logl_train_multi + \"\\t\\t\"\n",
    "#                           + \"%.2f\" % abs(logl_train_single - best_logl_train_multi) + \"\\t\\t\"\n",
    "#                           + \"%.2f\" % logl_test_single + \"\\t\\t\"\n",
    "#                           + \"%.2f\" % logl_test_multi + \"\\t\\t\"\n",
    "#                           + \"%.2f\" % abs(logl_test_single - logl_test_multi) + \"\\n\")\n",
    "\n",
    "        # latex table print\n",
    "        results_table += (str(P) + \" & \" + str(T) + \" & \" \n",
    "                          + \"%.2f\" % logl_train_single + \" & \"\n",
    "                          + \"%.2f\" % best_logl_train_multi + \" & \"\n",
    "                          + \"%.2f\" % (logl_train_single - best_logl_train_multi) + \" & \"\n",
    "                          + \"%.2f\" % logl_test_single + \" & \"\n",
    "                          + \"%.2f\" % logl_test_multi + \" & \"\n",
    "                          + \"%.2f\" % (logl_test_single - logl_test_multi) + \"\\\\\\\\\\n\")\n",
    "            \n",
    "print(\" \")\n",
    "print(\"DONE! Multi inits used \" + str(num_multi_inits) +\" initializations.\")\n",
    "print(\" \")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 5 - Compare with linear\n",
    "\n",
    "Compare the two models (simple linear model and mixture with two linear compo- nents). Do the analyses with both low (eg. 2) and high (eg. 10) data dimensionality as well as with small (eg. 10) and large (eg. 100) amount of samples. Use separate validation set as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dimensions_ex5 = [10, 2]\n",
    "datasamples_ex5 = [100, 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper function for multi-init train/validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_train(model_class, hyperp, X, Y, X_v, Y_v, inits=5, really=False):\n",
    "    # train\n",
    "    best_model = model_class(hyperp, X, Y)\n",
    "    i, best_logl_train, r = best_model.EM_fit()\n",
    "    for k in range(0, inits):\n",
    "        model_train = model_class(hyperp, X, Y)\n",
    "        i, logl_train, r = model_train.EM_fit()\n",
    "        if (logl_train > best_logl_train):\n",
    "            best_logl_train = logl_train\n",
    "            best_model = model_train\n",
    "\n",
    "    # test / validate\n",
    "    fit_params = best_model.get_p()\n",
    "    model_test = model_class(hyperp, X_v, Y_v)\n",
    "    model_test.set_p(fit_params)\n",
    "    if not really:\n",
    "        logl_test, ll = model_test.logl()\n",
    "    else:\n",
    "        logl_test, ll = model_test.logl(really=True)\n",
    "    \n",
    "    return best_logl_train, logl_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw data from the simple linear model, analyze how well each of the candidate models is able to explain the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "DONE! Drawn from linear model and results of linear model and mixture model\n",
      " \n",
      "P &T\tLM Train\t LM Test\tMM Train\t MM Test\t\n",
      "10 & 100 & -92.81 & -82.74 & -31.51 & -25.79 \\\\ \n",
      "10 & 10 & -10.87 & -2.51 & -13.81 & -4.10 \\\\ \n",
      "2 & 100 & -73.44 & -69.05 & -17.29 & -2.97 \\\\ \n",
      "2 & 10 & -2.37 & 4.07 & 0.10 & 7.03 \\\\ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwestrup/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "validation_set_size = 0.3\n",
    "inits = 5\n",
    "epoch = 0\n",
    "\n",
    "results_table = (\"P &\" + \"T\\t\" + \"LM Train\\t\" + \" LM Test\\t\"\n",
    "                 + \"MM Train\\t\" + \" MM Test\\t\" + \"\\n\")\n",
    "\n",
    "for P in dimensions_ex5:\n",
    "    for T in datasamples_ex5:\n",
    "        \n",
    "        # print progress\n",
    "        epoch += 1\n",
    "        total = len(dimensions_ex5) * len(datasamples_ex5) \n",
    "        percent = epoch/total\n",
    "        print(\"epoch \" + str(epoch) + \" / \" + str(total) \n",
    "              + \" : P=\" + str(P) + \" T=\" + str(T), end=\"\\r\")\n",
    "        \n",
    "        ## INIT\n",
    "        \n",
    "        # get hyperparameters for model\n",
    "        hyperp = get_hyperp()\n",
    "        # generate data\n",
    "        ndata = T\n",
    "        ndata_v = np.ceil(T * 0.3)\n",
    "        pdata = P\n",
    "        X = generate_X(ndata, pdata)\n",
    "        X_v = generate_X(ndata_v, pdata)\n",
    "\n",
    "        # generate true linear model\n",
    "        true_model = EM_algo_LM(hyperp, ndata=ndata, pdata=pdata)\n",
    "        Y, Z = generate_YZ(X, true_model)\n",
    "        Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "                    \n",
    "        ## Fit Linear Model:\n",
    "        LM_best_logl_train, LM_logl_test = test_train(EM_algo_LM, hyperp, X, Y, X_v, Y_v)\n",
    "        \n",
    "        ## Fit Mixture Model:\n",
    "        MM_best_logl_train, MM_logl_test = test_train(EM_algo_MM, hyperp, X, Y, X_v, Y_v, really=True)\n",
    "        \n",
    "        results_table += (str(P) + \" & \" + str(T) + \" & \" \n",
    "                          + (\"%.2f\" % LM_best_logl_train) + \" & \"\n",
    "                          + (\"%.2f\" % MM_best_logl_train) + \" & \"\n",
    "                          + (\"%.2f\" % LM_logl_test) + \" & \"\n",
    "                          + (\"%.2f\" % MM_logl_test) + \" \\\\\\\\ \" \n",
    "                          + \"\\n\")        \n",
    "\n",
    "print(\" \")\n",
    "print(\"DONE! Drawn from linear model and results of linear model and mixture model\")\n",
    "print(\" \")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw data from the mixture model, analyze how well each of the candidate models is able to explain the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "DONE! Drawn from mixture model and results of linear model and mixture model\n",
      " \n",
      "P &T\tLM Train\tMM Train\tLM Test\t MM Test\t\n",
      "10 & 100 & -129.27 & -78.85 & -63.96 & -131.84 \\\\ \n",
      "10 & 10 & -2.00 & 6.86 & -48.73 & -26.53 \\\\ \n",
      "2 & 100 & -120.92 & -108.22 & -38.02 & -16.98 \\\\ \n",
      "2 & 10 & -17.21 & -9.64 & -6.82 & 1.21 \\\\ \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cwestrup/anaconda/envs/py35/lib/python3.5/site-packages/ipykernel/__main__.py:9: VisibleDeprecationWarning: using a non-integer number instead of an integer will result in an error in the future\n"
     ]
    }
   ],
   "source": [
    "validation_set_size = 0.3\n",
    "inits = 5\n",
    "epoch = 0\n",
    "\n",
    "results_table = (\"P &\" + \"T\\t\" + \"LM Train\\t\" + \"MM Train\\t\"\n",
    "                 + \"LM Test\\t\" + \" MM Test\\t\" + \"\\n\")\n",
    "\n",
    "for P in dimensions_ex5:\n",
    "    for T in datasamples_ex5:\n",
    "        \n",
    "        # print progress\n",
    "        epoch += 1\n",
    "        total = len(dimensions_ex5) * len(datasamples_ex5)\n",
    "        percent = epoch/total\n",
    "        print(\"epoch \" + str(epoch) + \" / \" + str(total) \n",
    "              + \" : P=\" + str(P) + \" T=\" + str(T), end=\"\\r\")\n",
    "        \n",
    "        ## INIT\n",
    "        \n",
    "        # get hyperparameters for model\n",
    "        hyperp = get_hyperp()\n",
    "        # generate data\n",
    "        ndata = T\n",
    "        ndata_v = np.ceil(T * 0.3)\n",
    "        pdata = P\n",
    "        X = generate_X(ndata, pdata)\n",
    "        X_v = generate_X(ndata_v, pdata)\n",
    "\n",
    "        # generate true linear model\n",
    "        true_model = EM_algo_MM(hyperp, ndata=ndata, pdata=pdata)\n",
    "        Y, Z = generate_YZ(X, true_model)\n",
    "        Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "                    \n",
    "        ## Fit Linear Model:\n",
    "        LM_best_logl_train, LM_logl_test = test_train(EM_algo_LM, hyperp, X, Y, X_v, Y_v)\n",
    "        \n",
    "        ## Fit Mixture Model:\n",
    "        MM_best_logl_train, MM_logl_test = test_train(EM_algo_MM, hyperp, X, Y, X_v, Y_v, really=True)\n",
    "        \n",
    "        results_table += (str(P) + \" & \" + str(T) + \" & \" \n",
    "                          + (\"%.2f\" % LM_best_logl_train) + \" & \"\n",
    "                          + (\"%.2f\" % MM_best_logl_train) + \" & \"\n",
    "                          + (\"%.2f\" % LM_logl_test) + \" & \"\n",
    "                          + (\"%.2f\" % MM_logl_test) + \" \\\\\\\\ \" \n",
    "                          + \"\\n\")        \n",
    "\n",
    "print(\" \")\n",
    "print(\"DONE! Drawn from mixture model and results of linear model and mixture model\")\n",
    "print(\" \")\n",
    "print(results_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw data from the mixture model, analyze which candidate model is able to explain the data better as a function of the similarity of the two linear components in the true model (e.g. cosine similarity). Explain your findings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1196.078125px",
   "right": "20px",
   "top": "122px",
   "width": "186px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
