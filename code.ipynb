{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import outer, eye, ones, zeros, diag, log, sqrt, exp, pi\n",
    "from numpy.linalg import inv, solve\n",
    "from numpy.random import multivariate_normal as mvnormal, normal, gamma, beta, binomial\n",
    "from scipy.special import gammaln\n",
    "from scipy.stats import norm\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "import copy\n",
    "\n",
    "class EM_algo():\n",
    "    \"\"\"\n",
    "        A superclass for different EM-fitted models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparams, X=None, Y=None, ndata=0, pdata=0):\n",
    "        \"\"\"\n",
    "            Initialize model based either on given data (X, Y) or\n",
    "            on given data dimensionality (ndata, pdata).\n",
    "        \"\"\"\n",
    "        if X != None and Y != None:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "            self.ndata = len(self.X)\n",
    "            self.pdata = len(self.X[0])\n",
    "        if ndata and pdata:\n",
    "            self.X = None\n",
    "            self.Y = None\n",
    "            self.ndata = ndata\n",
    "            self.pdata = pdata\n",
    "        self.h = hyperparams\n",
    "        self.p = dict() # model parameters\n",
    "        self.reset()\n",
    "        if X != None and Y != None:\n",
    "            self.current_logl, self.cll = self.logl()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the drawn y and z-values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_fit(self, alim=1e-10, maxit=1e4):\n",
    "        \"\"\"\n",
    "            Calls the EM_iter repeatedly until the log likelihood\n",
    "            of the model increases less than 'alim' in absolute\n",
    "            value or after 'maxit' iterations have been done.\n",
    "\n",
    "            Returns the number of EM-iterations, final log likelihood\n",
    "            value and a string that explains the end condition.\n",
    "        \"\"\"\n",
    "        logl, ll = self.logl()\n",
    "        for i in range(int(maxit)):\n",
    "            self.EM_iter()\n",
    "            logl2, ll2 = self.logl()\n",
    "            adiff = abs(logl2 - logl)\n",
    "            if adiff < alim:\n",
    "                return i+1, logl2, \"alim\"\n",
    "            logl = logl2\n",
    "        return maxit, logl2, \"maxit\"\n",
    "\n",
    "\n",
    "    def assert_logl_increased(self, event):\n",
    "        \"\"\"\n",
    "            Checks that the log likelihood increased since model\n",
    "            initialization or the time this function was last called.\n",
    "        \"\"\"\n",
    "        newlogl, ll = self.logl()\n",
    "        if self.current_logl - newlogl > 1e-3:\n",
    "            self.debug_logl(self.cll, ll)\n",
    "            raise ValueError(\"logl increased after %s\" % (event))\n",
    "        self.current_logl, self.cll = newlogl, ll\n",
    "\n",
    "\n",
    "    def get_p(self):\n",
    "        \"\"\"\n",
    "            Returns a copy of the model parameters.\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.p)\n",
    "\n",
    "\n",
    "    def set_p(self, p):\n",
    "        \"\"\"\n",
    "            Sets the model parameters.\n",
    "        \"\"\"\n",
    "        self.p = p.copy()\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        for k, v in self.p.items():\n",
    "            print(\"%s = %s\" % (k, v))\n",
    "\n",
    "\n",
    "    def pretty_vector(self, x):\n",
    "        \"\"\"\n",
    "            Returns a formatted version of a vector.\n",
    "        \"\"\"\n",
    "        s = [\"(\"]\n",
    "        s.extend([\"%.2f, \" % (xi) for xi in x[:-1]])\n",
    "        s.append(\"%.2f)\" % (x[-1]))\n",
    "        return \"\".join(s)\n",
    "\n",
    "\n",
    "    def debug_logl(self, ll1, ll2):\n",
    "        \"\"\"\n",
    "            Prints an analysis of the per-term change in\n",
    "            log likelihood from ll1 to ll2.\n",
    "        \"\"\"\n",
    "        print(\"Logl      before     after\")\n",
    "        for v1, v2, i in zip(ll1, ll2, range(len(ll1))):\n",
    "            if v1 > v2:\n",
    "                d = \">\"\n",
    "            elif v2 > v1:\n",
    "                d = \"<\"\n",
    "            else:\n",
    "                d = \"=\"\n",
    "            print(\"Term %02d: %7.3f %s %7.3f\" % (i, v1, d, v2))\n",
    "        print(\"Total    %7.3f   %7.3f\" % (sum(ll1), sum(ll2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "from numpy import outer, eye, ones, zeros, log, sqrt, exp, pi\n",
    "from numpy.linalg import inv, solve\n",
    "from numpy.random import multivariate_normal as mvnormal, normal, gamma, beta, binomial\n",
    "from scipy.special import gammaln\n",
    "\n",
    "class EM_algo_LM(EM_algo):\n",
    "    \"\"\"\n",
    "        A linear gaussian model.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "\n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p           = dict()\n",
    "        self.p[\"sigma2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value (and a constant z-value for compatibility)\n",
    "        \"\"\"\n",
    "        mean = float(item.dot(self.p[\"phi\"]))\n",
    "        std  = sqrt(self.p[\"sigma2\"])\n",
    "        return normal(mean, std), 1\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll    = zeros(8)\n",
    "        phie  = self.p[\"phi\"] - self.mu_phi0\n",
    "        err   = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        # p(y)\n",
    "        ll[0] = - 0.5 * log(2 * pi * self.p[\"sigma2\"]) * self.ndata\n",
    "        ll[1] = sum(- 0.5 * err / self.p[\"sigma2\"])\n",
    "        # p(phi)\n",
    "        ll[2] = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2\"]) * self.pdata\n",
    "        ll[3] = - 0.5 * phie.T.dot(phie) / (self.lbd_phi0 * self.p[\"sigma2\"])\n",
    "        # p(sigma2)\n",
    "        ll[4] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[5] = - gammaln(self.alpha_s20)\n",
    "        ll[6] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2\"])\n",
    "        ll[7] = - self.beta_s20 / self.p[\"sigma2\"]\n",
    "        return sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        # phi\n",
    "        sumxx         = self.X.T.dot(self.X)\n",
    "        sumxy         = self.X.T.dot(self.Y)\n",
    "        sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "        self.p[\"phi\"] = solve(sigma_phi_inv, sigma_mu + sumxy)\n",
    "        self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "        # sigma2\n",
    "        phie = (self.p[\"phi\"] - self.mu_phi0) ** 2\n",
    "        err  = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        num  = self.beta_s20 + 0.5 * sum(err) + 0.5 * sum(phie) / self.lbd_phi0\n",
    "        den  = self.alpha_s20 + 1.0 + 0.5 * (self.ndata + self.pdata)\n",
    "        self.p[\"sigma2\"] = num / den\n",
    "        if self.p[\"sigma2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2 < 0.0\")\n",
    "        self.assert_logl_increased(\"sigma2 update\")\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        print(\"phi    : %s\" % (self.pretty_vector(self.p[\"phi\"])))\n",
    "        print(\"sigma2 : %.3f\" % (self.p[\"sigma2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy.random import randn\n",
    "\n",
    "\n",
    "def generate_X(ndata, pdata):\n",
    "    \"\"\"\n",
    "        Return a matrix of normally distributed random values.\n",
    "    \"\"\"\n",
    "    X = randn(ndata, pdata)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_YZ(X, distribution):\n",
    "    \"\"\"\n",
    "        Draw observations Y and latent variable values Z from a distribution.\n",
    "    \"\"\"\n",
    "    ndata = len(X)\n",
    "    Y = zeros(ndata)\n",
    "    Z = zeros(ndata)\n",
    "    for i in range(ndata):\n",
    "        Y[i], Z[i] = distribution.draw(X[i])\n",
    "    return Y, Z\n",
    "\n",
    "\n",
    "def get_hyperp():\n",
    "    \"\"\"\n",
    "        Return model hyperparameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "            \"alpha_s20\": 5.0,\n",
    "            \"beta_s20\" : 1.0,\n",
    "            \"lbd_phi0\" : 1.0,\n",
    "            \"mu_phi0\"  : 0.0,\n",
    "            \"alpha_w0\" : 3.0,\n",
    "            \"beta_w0\"  : 3.0,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for responsibilities \n",
    "$$\\gamma_1 =  \\frac{ w_S \\mathcal{N}(y_t \\ | \\ x_t \\phi_{1_S}, \\sigma_{1_S}^2)}{ w_S \\mathcal{N}(y_t \\ | \\ x_t \\phi_{1_S} , \\sigma_{1_S}^2) \\; + \\; (1 - w_S) \\mathcal{N}(y_t \\ | \\ x_t \\phi_{2_S}, \\sigma_{2_S}^2)} $$\n",
    "$$\\gamma_2 =  \\frac{ (1 - w_S) \\mathcal{N}(y_t \\ | \\ x_t \\phi_{2_S}, \\sigma_{2_S}^2)}{ w_S \\mathcal{N}(y_t \\ | \\ x_t \\phi_{1_S} , \\sigma_{1_S}^2) \\; + \\; (1 - w_S) \\mathcal{N}(y_t \\ | \\ x_t \\phi_{2_S}, \\sigma_{2_S}^2)} $$\n",
    "\n",
    "If we only need to know whether $\\gamma_1 > \\gamma_2$, then we need to only compute\n",
    "\n",
    "$$ w_S \\mathcal{N}(y_t \\ | \\ x_t \\phi_{1_S}, \\sigma_{1_S}^2) > (1 - w_S) \\mathcal{N}(y_t \\ | \\ x_t \\phi_{2_S}, \\sigma_{2_S}^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import outer, eye, ones, zeros, diag, log, sqrt, exp, pi\n",
    "from numpy.linalg import inv, solve\n",
    "from numpy.random import multivariate_normal as mvnormal, normal, gamma, beta, binomial\n",
    "from scipy.special import gammaln\n",
    "from scipy.stats import norm\n",
    "import numpy as np\n",
    "\n",
    "class EM_algo_MM(EM_algo):\n",
    "    \"\"\"\n",
    "        A mixture of two linear models.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.alpha_w0       = self.h[\"alpha_w0\"]\n",
    "        self.beta_w0        = self.h[\"beta_w0\"]\n",
    "\n",
    "        # Same priors for phi1 and phi2, s2_1, s2_2, don't bother to copy vars twice\n",
    "        # i.e. alpha_s2_1_0 = alpha_s2_2_0 = alpha_s20\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "        \n",
    "        # Precalculations:\n",
    "        self.w_gamma_ln_multiplier  = gammaln(self.alpha_w0 + self.beta_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.alpha_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.beta_w0)\n",
    "        \n",
    "        \n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p             = dict()\n",
    "        # Weights\n",
    "        self.p[\"w\"]        = beta(self.alpha_w0, self.beta_w0)\n",
    "        # Responsibilities (TODO: do we need this here?)\n",
    "        self.p[\"gamma\"]    = binomial(1, self.p[\"w\"], self.ndata)\n",
    "        # Component 1\n",
    "        self.p[\"sigma2_1\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_1\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_1\"] * self.sigma_phi0)\n",
    "        # Component 2\n",
    "        self.p[\"sigma2_2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_2\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value and z-value\n",
    "        \"\"\"    \n",
    "        mean1 = float(item.dot(self.p[\"phi_1\"]))\n",
    "        std1  = sqrt(self.p[\"sigma2_1\"])\n",
    "        mean2 = float(item.dot(self.p[\"phi_2\"]))\n",
    "        std2  = sqrt(self.p[\"sigma2_2\"])\n",
    "        \n",
    "        # Responsibilites for item (item is a data point)\n",
    "        propto_gamma1 = self.p[\"w\"] * norm.pdf(item, mean1, std1)\n",
    "        propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(item, mean2, std2)\n",
    "        \n",
    "        if (propto_gamma1 > propto_gamma2)\n",
    "            return normal(mean1, std1), 1\n",
    "        else\n",
    "            return normal(mean2, std2), 2\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll         = zeros(20)\n",
    "        phi_1_diff = self.p[\"phi_1\"] - self.mu_phi0\n",
    "        phi_2_diff = self.p[\"phi_2\"] - self.mu_phi0\n",
    "        phi_1_err  = phi_1_diff.T.dot(phi_1_diff)\n",
    "        phi_2_err  = phi_2_diff.T.dot(phi_2_diff)\n",
    "        err_1      = (self.X.dot(self.p[\"phi_1\"]) - self.Y) ** 2\n",
    "        err_2      = (self.X.dot(self.p[\"phi_2\"]) - self.Y) ** 2\n",
    "        \n",
    "        \n",
    "        # Responsibilities\n",
    "#         mean1 = float(item.dot(self.p[\"phi_1\"]))\n",
    "#         std1  = sqrt(self.p[\"sigma2_1\"])\n",
    "#         mean2 = float(item.dot(self.p[\"phi_2\"]))\n",
    "#         std2  = sqrt(self.p[\"sigma2_2\"])\n",
    "#         # data is 1 dimensional, and norm.pdf works on a vector returning probability for each separately\n",
    "#         propto_gamma1 = self.p[\"w\"] * norm.pdf(self.X, mean1, std1)\n",
    "#         propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.X, mean2, std2)\n",
    "#         # elementwise, works because numpy\n",
    "#         gamma = propto_gamma1 / (propto_gamma1 + propto_gamma2)\n",
    "        gamma = self.p[\"gamma\"]\n",
    "        \n",
    "        ### posterior factorizes p(y,z,w,phi,sigma) = p(y)p(z)p(w)p(phi)p(sigma)\n",
    "        \n",
    "        ### p(y)\n",
    "        ll[0] = self.p[\"w\"]       * (-0.5 * log(2 * pi * self.p[\"sigma2_1\"]) * self.ndata)\n",
    "        ll[1] = self.p[\"w\"]       * sum(- 0.5 * err_1 / self.p[\"sigma2_1\"])\n",
    "        ll[2] = (1 - self.p[\"w\"]) * (-0.5 * log(2 * pi * self.p[\"sigma2_2\"]) * self.ndata)\n",
    "        ll[3] = (1 - self.p[\"w\"]) * sum(- 0.5 * err_2 / self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(z)\n",
    "        ll[4] = (gamma * log(self.p[\"w\"])) + ((1 - gamma) * log(1 - self.p[\"w\"]))\n",
    "        \n",
    "        ### p(w)\n",
    "        ll[5] = self.w_gamma_ln_multiplier\n",
    "        ll[6] = (self.alpha_w0 - 1) * self.p[\"w\"]\n",
    "        ll[7] = (self.beta_w0  - 1) * (1 - self.p[\"w\"])\n",
    "        \n",
    "        ### p(phi)\n",
    "        # phi_1\n",
    "        ll[8]  = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2_1\"]) * self.pdata\n",
    "        ll[9]  = - 0.5 * phi_1_err / (self.lbd_phi0 * self.p[\"sigma2_1\"])\n",
    "        # phi_2\n",
    "        ll[10] = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2_2\"]) * self.pdata\n",
    "        ll[11] = - 0.5 * phi_2_err / (self.lbd_phi0 * self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(sigma2)\n",
    "        # sigma2_1\n",
    "        ll[12] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[13] = - gammaln(self.alpha_s20)\n",
    "        ll[14] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_1\"])\n",
    "        ll[15] = - self.beta_s20 / self.p[\"sigma2_1\"]\n",
    "        # sigma2_2\n",
    "        ll[16] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[17] = - gammaln(self.alpha_s20)\n",
    "        ll[18] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_2\"])\n",
    "        ll[19] = - self.beta_s20 / self.p[\"sigma2_2\"]\n",
    "        \n",
    "        return sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        # self.ndata = T\n",
    "        # self.pdata = P\n",
    "        \n",
    "        # phi1\n",
    "#         sumxx          = self.X.T.dot(self.X)\n",
    "#         sumxy          = self.X.T.dot(self.Y)\n",
    "#         sigma_mu       = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "#         sigma_phi_inv  = self.sigma_phi0_inv + sumxx\n",
    "#         self.p[\"phi1\"] = solve(sigma_phi1_inv, sigma_mu + sumxy)\n",
    "#         self.assert_logl_increased(\"phi1 update\")\n",
    "\n",
    "        # ==================== E-STEP ====================\n",
    "\n",
    "        # ========== Responsibilities gamma ==========\n",
    "        mean1 = float(item.dot(self.p[\"phi_1\"]))\n",
    "        std1  = sqrt(self.p[\"sigma2_1\"])\n",
    "        mean2 = float(item.dot(self.p[\"phi_2\"]))\n",
    "        std2  = sqrt(self.p[\"sigma2_2\"])\n",
    "\n",
    "        # data is 1 dimensional, and norm.pdf works on a vector returning probability for each separately\n",
    "        propto_gamma1 = self.p[\"w\"] * norm.pdf(self.X, mean1, std1)\n",
    "        propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.X, mean2, std2)\n",
    "        \n",
    "        # elementwise, works because numpy\n",
    "        gamma = propto_gamma1 / (propto_gamma1 + propto_gamma2)\n",
    "        self.p[\"gamma\"] = gamma\n",
    "        sum_gamma = sum(gamma)\n",
    "#         gamma2 = 1-gamma\n",
    "        \n",
    "#         gamma = zeros(self.ndata)\n",
    "#         for t in range(self.ndata):\n",
    "#             propto_gamma1 = self.p[\"w\"] * norm.pdf(self.X[t], mean1, std1)\n",
    "#             propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.X[t], mean2, std2)\n",
    "#             gamma[t] = propto_gamma1 / (propto_gamma1 + propto_gamma2)\n",
    "\n",
    "        # ==================== M-STEP ====================\n",
    "\n",
    "        # ========== Weights w ==========\n",
    "        num = sum_gamma + self.alpha_w0 - 1\n",
    "        den = self.ndata - 2*sum_gamma - self.alpha_w0 - self.beta_w0 + 2\n",
    "        self.p[\"w\"] = 2 + (num / den)\n",
    "        \n",
    "        \n",
    "        # ========== Variables phi ==========\n",
    "        \n",
    "#         sum_gammaxx   = sum(np.multiply(gamma, np.multiply(self.X, self.X)))\n",
    "#         sum_gammaxy   = sum(np.multiply(gamma, np.multiply(self.X, self.Y)))\n",
    "        sum_gammaxx   = np.dot(gamma, np.multiply(self.X, self.X))\n",
    "        sum_gammaxy   = np.dot(gamma * self.Y, self.X)\n",
    "        # ?????\n",
    "        sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "        \n",
    "        # phi_1\n",
    "        \n",
    "        \n",
    "        # From linear model\n",
    "#         sumxx         = self.X.T.dot(self.X)\n",
    "#         sumxy         = self.X.T.dot(self.Y)\n",
    "#         sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "#         sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "#         self.p[\"phi\"] = solve(sigma_phi_inv, sigma_mu + sumxy)\n",
    "#         self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "\n",
    "        # ========== Variances sigma2 ==========\n",
    "        \n",
    "        # sigma2_1\n",
    "        err  = (self.Y - self.X.dot(self.p[\"phi1\"])) ** 2\n",
    "        num  = self.beta_s20 + 0.5 * sum(gamma * err)\n",
    "        den  = self.alpha_s20 + 1.0 + 0.5\n",
    "        self.p[\"sigma2_1\"] = num / den\n",
    "        if self.p[\"sigma2_1\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_1 < 0.0\")\n",
    "        \n",
    "        # sigma2_2\n",
    "        err  = (self.Y - self.X.dot(self.p[\"phi2\"])) ** 2\n",
    "        num  = self.beta_s20 + 0.5 * sum((1-gamma) * err)\n",
    "        den  = self.alpha_s20 + 1.0 + 0.5\n",
    "        self.p[\"sigma2_2\"] = num / den\n",
    "        if self.p[\"sigma2_2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_2 < 0.0\")\n",
    "        \n",
    "        self.assert_logl_increased(\"sigma2 update\")\n",
    "\n",
    "\n",
    "        \n",
    "#         # phi\n",
    "#         sumxx         = self.X.T.dot(self.X)\n",
    "#         sumxy         = self.X.T.dot(self.Y)\n",
    "#         sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "#         sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "#         self.p[\"phi\"] = solve(sigma_phi_inv, sigma_mu + sumxy)\n",
    "#         self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "#         # sigma2\n",
    "#         phie = (self.p[\"phi\"] - self.mu_phi0) ** 2\n",
    "#         err  = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "#         num  = self.beta_s20 + 0.5 * sum(err) + 0.5 * sum(phie) / self.lbd_phi0\n",
    "#         den  = self.alpha_s20 + 1.0 + 0.5 * (self.ndata + self.pdata)\n",
    "#         self.p[\"sigma2\"] = num / den\n",
    "#         if self.p[\"sigma2\"] < 0.0:\n",
    "#             raise ValueError(\"sigma2 < 0.0\")\n",
    "#         self.assert_logl_increased(\"sigma2 update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.,  0.],\n",
       "       [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, min, max, sqrt, mean, std\n",
    "from scipy.spatial.distance import cosine\n",
    "import numpy as np\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        Executed when program is run.\n",
    "    \"\"\"\n",
    "    print(\"Starting program\")\n",
    "    print(\"\")\n",
    "    test_LM_model()\n",
    "\n",
    "\n",
    "def test_LM_model():\n",
    "    \"\"\"\n",
    "        Example that demonstrates how to call the model.\n",
    "    \"\"\"\n",
    "    # get hyperparameters for model\n",
    "    hyperp = get_hyperp()\n",
    "    # generate 50 training data and 20 validation data locations of dim=1\n",
    "    ndata = 50\n",
    "    ndata_v = 50\n",
    "    pdata = 1\n",
    "    X = generate_X(ndata, pdata)\n",
    "    X_v = generate_X(ndata_v, pdata)\n",
    "    # intialize true model randomly and draw observations from it\n",
    "    true_model = EM_algo_LM(hyperp, ndata=ndata, pdata=pdata)\n",
    "    Y, Z = generate_YZ(X, true_model)\n",
    "    Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "    print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "            (ndata, ndata_v))\n",
    "    true_model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # generate a model for estimating the parameters of the\n",
    "    # true model based on the observations (X, Y) we just made\n",
    "    model = EM_algo_LM(hyperp, X, Y)\n",
    "    i, logl, r = model.EM_fit()\n",
    "    print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "            (logl, i, r))\n",
    "    print(\"\")\n",
    "    print(\"MAP estimate of true model parameters:\")\n",
    "    model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # crossvalidate the estimated model with the validation data\n",
    "    fit_params = model.get_p()\n",
    "    model_v = EM_algo_LM(hyperp, X_v, Y_v)\n",
    "    model_v.set_p(fit_params)\n",
    "    logl, ll = model_v.logl()\n",
    "    print(\"Crossvalidated logl: %.2f\" % (logl))\n",
    "\n",
    "    # if possible, plot samples, true model and estimated model\n",
    "    if pdata != 1:\n",
    "        return\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "    plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "    print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "            true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "    print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "            model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def print_linear_model(x, phi, sigma2, color, label):\n",
    "    \"\"\"\n",
    "        Print linear model mean and 95% confidence interval.\n",
    "    \"\"\"\n",
    "    y = phi * x\n",
    "    plt.plot(x, y, color, label=label)\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(sigma2), y - 1.96 * sqrt(sigma2), \\\n",
    "            alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = generate_X(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.22470801,  0.35364688],\n",
       "       [-0.29730505,  0.86804445],\n",
       "       [-0.51008941,  0.27406517],\n",
       "       [-0.90501391,  1.43689847],\n",
       "       [-0.16736504,  0.304238  ],\n",
       "       [ 0.07960696,  0.17671287],\n",
       "       [-1.37105112, -1.1726178 ],\n",
       "       [ 0.67606752, -0.06235661],\n",
       "       [ 0.54192738,  0.41945852],\n",
       "       [-0.22212765, -0.15550222]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.54192738,  0.41945852])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.1884555 ,  0.3747592 ],\n",
       "       [ 0.3816949 ,  0.27370918],\n",
       "       [ 0.3502759 ,  0.38423751],\n",
       "       [ 0.26488391,  0.14209248],\n",
       "       [ 0.39339383,  0.38089981],\n",
       "       [ 0.39768018,  0.39276168],\n",
       "       [ 0.15585501,  0.20059752],\n",
       "       [ 0.31743818,  0.39816742],\n",
       "       [ 0.34445867,  0.3653457 ],\n",
       "       [ 0.38922065,  0.39414792]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm.pdf(X, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p_1 = np.array([0.3, 0.2])\n",
    "p_2 = np.array([0.2, 0.8])\n",
    "asd = p_1 / (p_1 + p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.6,  0.2])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.3 / (0.3+0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.2 / (0.2+0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.38176416,  0.52539716],\n",
       "       [ 0.52539716,  4.72119164]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.06,  0.16])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.multiply(p_1, p_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_model = EM_algo_LM(get_hyperp(), ndata=10, pdata=2)\n",
    "Y, Z = generate_YZ(X, true_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.92646461,  1.47475573])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?np.dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.38176416,  0.52539716],\n",
       "       [ 0.52539716,  4.72119164]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.38176416,  0.        ],\n",
       "       [ 0.        ,  4.72119164]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eye(2) * np.sum(np.multiply(X, X),axis=0,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.38176416,  0.52539716],\n",
       "       [ 0.52539716,  4.72119164]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04049531,  1.82697251])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(Y, np.multiply(X, X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total = np.array([0.0, 0.0])\n",
    "for t in range(10):\n",
    "    total += Y[t] * X[t] * X[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.04049531,  1.82697251])"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "asd1=Y * Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asd1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16473486,  0.96779041])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.dot(asd1, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "total2 = np.array([0.0, 0.0])\n",
    "for t in range(10):\n",
    "    total2 += Y[t] * X[t] * Y[t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.16473486,  0.96779041])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.38176416,  0.52539716],\n",
       "       [ 0.52539716,  4.72119164]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
