{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import outer, eye, ones, zeros, diag, log, sqrt, exp, pi\n",
    "from numpy.linalg import inv, solve\n",
    "from numpy.random import multivariate_normal as mvnormal, normal, gamma, beta, binomial\n",
    "from scipy.special import gammaln\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy.random import randn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, min, max, sqrt, mean, std\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'invalid': 'warn', 'over': 'warn', 'under': 'ignore'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.seterr(all='raise') # TODO REMOVE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "mm_cumulative_error = 0.0 # TODO REMOVE\n",
    "mm_max_diff = 0.0 # TODO REMOVE\n",
    "\n",
    "class EM_algo():\n",
    "    \"\"\"\n",
    "        A superclass for different EM-fitted models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparams, X=None, Y=None, ndata=0, pdata=0):\n",
    "        \"\"\"\n",
    "            Initialize model based either on given data (X, Y) or\n",
    "            on given data dimensionality (ndata, pdata).\n",
    "        \"\"\"\n",
    "        if not X is None and not Y is None:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "            self.ndata = len(self.X)\n",
    "            self.pdata = len(self.X[0])\n",
    "        if ndata and pdata:\n",
    "            self.X = None\n",
    "            self.Y = None\n",
    "            self.ndata = ndata\n",
    "            self.pdata = pdata\n",
    "        self.h = hyperparams\n",
    "        self.p = dict() # model parameters\n",
    "        self.reset()\n",
    "        if not X is None and not Y is None:\n",
    "            self.current_logl, self.cll = self.logl()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the drawn y and z-values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_fit(self, alim=1e-10, maxit=1e4):\n",
    "        \"\"\"\n",
    "            Calls the EM_iter repeatedly until the log likelihood\n",
    "            of the model increases less than 'alim' in absolute\n",
    "            value or after 'maxit' iterations have been done.\n",
    "\n",
    "            Returns the number of EM-iterations, final log likelihood\n",
    "            value and a string that explains the end condition.\n",
    "        \"\"\"\n",
    "        logl, ll = self.logl()\n",
    "        for i in range(int(maxit)):\n",
    "            self.EM_iter()\n",
    "            logl2, ll2 = self.logl()\n",
    "            adiff = abs(logl2 - logl)\n",
    "            if adiff < alim:\n",
    "                return i+1, logl2, \"alim\"\n",
    "            logl = logl2\n",
    "        return maxit, logl2, \"maxit\"\n",
    "\n",
    "\n",
    "    def assert_logl_increased(self, event):\n",
    "        \"\"\"\n",
    "            Checks that the log likelihood increased since model\n",
    "            initialization or the time this function was last called.\n",
    "        \"\"\"\n",
    "        newlogl, ll = self.logl()\n",
    "        global mm_cumulative_error # TODO REMOVE\n",
    "        global mm_max_diff # TODO REMOVE\n",
    "        if self.current_logl > newlogl:\n",
    "            if mm_max_diff < self.current_logl - newlogl:\n",
    "                mm_max_diff = self.current_logl - newlogl\n",
    "            mm_cumulative_error = mm_cumulative_error + (self.current_logl - newlogl)\n",
    "        if self.current_logl - newlogl > 0.1:\n",
    "#         if self.current_logl - newlogl > 1e-3:\n",
    "            raise ValueError(\"logl decreased after %s\" % (event))\n",
    "        self.current_logl, self.cll = newlogl, ll\n",
    "\n",
    "\n",
    "    def get_p(self):\n",
    "        \"\"\"\n",
    "            Returns a copy of the model parameters.\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.p)\n",
    "\n",
    "\n",
    "    def set_p(self, p):\n",
    "        \"\"\"\n",
    "            Sets the model parameters.\n",
    "        \"\"\"\n",
    "        self.p = p.copy()\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        for k, v in self.p.items():\n",
    "            print(\"%s = %s\" % (k, v))\n",
    "\n",
    "\n",
    "    def pretty_vector(self, x):\n",
    "        \"\"\"\n",
    "            Returns a formatted version of a vector.\n",
    "        \"\"\"\n",
    "        s = [\"(\"]\n",
    "        s.extend([\"%.2f, \" % (xi) for xi in x[:-1]])\n",
    "        s.append(\"%.2f)\" % (x[-1]))\n",
    "        return \"\".join(s)\n",
    "\n",
    "\n",
    "    def debug_logl(self, ll1, ll2):\n",
    "        \"\"\"\n",
    "            Prints an analysis of the per-term change in\n",
    "            log likelihood from ll1 to ll2.\n",
    "        \"\"\"\n",
    "        print(\"Logl      before     after\")\n",
    "        for v1, v2, i in zip(ll1, ll2, range(len(ll1))):\n",
    "            if v1 > v2:\n",
    "                d = \">\"\n",
    "            elif v2 > v1:\n",
    "                d = \"<\"\n",
    "            else:\n",
    "                d = \"=\"\n",
    "            print(\"Term %02d: %7.3f %s %7.3f\" % (i, v1, d, v2))\n",
    "        v1 = sum(ll1)\n",
    "        v2 = sum(ll2)\n",
    "        if v1 > v2:\n",
    "            d = \">\"\n",
    "        elif v2 > v1:\n",
    "            d = \"<\"\n",
    "        else:\n",
    "            d = \"=\"\n",
    "        diff = v2-v1\n",
    "        print(\"Total    %7.3f %s %7.3f   diff: %7.3f\" % (v1, d, v2, diff))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "class EM_algo_LM(EM_algo):\n",
    "    \"\"\"\n",
    "        A linear gaussian model.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "\n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p           = dict()\n",
    "        self.p[\"sigma2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value (and a constant z-value for compatibility)\n",
    "        \"\"\"\n",
    "        mean = float(item.dot(self.p[\"phi\"]))\n",
    "        std  = sqrt(self.p[\"sigma2\"])\n",
    "        return normal(mean, std), 1\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll    = zeros(8)\n",
    "        phie  = self.p[\"phi\"] - self.mu_phi0\n",
    "        err   = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        # p(y)\n",
    "        ll[0] = - 0.5 * log(2 * pi * self.p[\"sigma2\"]) * self.ndata\n",
    "        ll[1] = sum(- 0.5 * err / self.p[\"sigma2\"])\n",
    "        # p(phi)\n",
    "        ll[2] = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2\"]) * self.pdata\n",
    "        ll[3] = - 0.5 * phie.T.dot(phie) / (self.lbd_phi0 * self.p[\"sigma2\"])\n",
    "        # p(sigma2)\n",
    "        ll[4] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[5] = - gammaln(self.alpha_s20)\n",
    "        ll[6] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2\"])\n",
    "        ll[7] = - self.beta_s20 / self.p[\"sigma2\"]\n",
    "        return sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        # phi\n",
    "        sumxx         = self.X.T.dot(self.X)\n",
    "        sumxy         = self.X.T.dot(self.Y)\n",
    "        sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "        self.p[\"phi\"] = solve(sigma_phi_inv, sigma_mu + sumxy)\n",
    "        self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "        # sigma2\n",
    "        phie = (self.p[\"phi\"] - self.mu_phi0) ** 2\n",
    "        err  = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        num  = self.beta_s20 + 0.5 * sum(err) + 0.5 * sum(phie) / self.lbd_phi0\n",
    "        den  = self.alpha_s20 + 1.0 + 0.5 * (self.ndata + self.pdata)\n",
    "        self.p[\"sigma2\"] = num / den\n",
    "        if self.p[\"sigma2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2 < 0.0\")\n",
    "        self.assert_logl_increased(\"sigma2 update\")\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        print(\"phi    : %s\" % (self.pretty_vector(self.p[\"phi\"])))\n",
    "        print(\"sigma2 : %.3f\" % (self.p[\"sigma2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "def generate_X(ndata, pdata):\n",
    "    \"\"\"\n",
    "        Return a matrix of normally distributed random values.\n",
    "    \"\"\"\n",
    "    X = randn(ndata, pdata)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_YZ(X, distribution):\n",
    "    \"\"\"\n",
    "        Draw observations Y and latent variable values Z from a distribution.\n",
    "    \"\"\"\n",
    "    ndata = len(X)\n",
    "    Y = zeros(ndata)\n",
    "    Z = zeros(ndata)\n",
    "    for i in range(ndata):\n",
    "        Y[i], Z[i] = distribution.draw(X[i])\n",
    "    return Y, Z\n",
    "\n",
    "\n",
    "def get_hyperp():\n",
    "    \"\"\"\n",
    "        Return model hyperparameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "            \"alpha_s20\": 5.0,\n",
    "            \"beta_s20\" : 1.0,\n",
    "            \"lbd_phi0\" : 1.0,\n",
    "            \"mu_phi0\"  : 0.0,\n",
    "            \"alpha_w0\" : 3.0,\n",
    "            \"beta_w0\"  : 3.0,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EM_algo_MM(EM_algo):\n",
    "    \"\"\"\n",
    "        A mixture of two linear models.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.alpha_w0       = self.h[\"alpha_w0\"]\n",
    "        self.beta_w0        = self.h[\"beta_w0\"]\n",
    "\n",
    "        # Same priors for phi1 and phi2, s2_1, s2_2, don't bother to copy vars twice\n",
    "        # i.e. alpha_s2_1_0 = alpha_s2_2_0 = alpha_s20\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "        \n",
    "        # Precalculations:\n",
    "        self.w_gamma_ln_multiplier  = gammaln(self.alpha_w0 + self.beta_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.alpha_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.beta_w0)\n",
    "        \n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p             = dict()\n",
    "        # Weights\n",
    "        self.p[\"w\"]        = beta(self.alpha_w0, self.beta_w0)\n",
    "        # Responsibilities\n",
    "        self.gamma         = binomial(1, self.p[\"w\"], self.ndata)\n",
    "        # Component 1\n",
    "        self.p[\"sigma2_1\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_1\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_1\"] * self.sigma_phi0)\n",
    "        # Component 2\n",
    "        self.p[\"sigma2_2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_2\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value and z-value\n",
    "        \"\"\"\n",
    "        mean1 = float(item.dot(self.p[\"phi_1\"]))\n",
    "        std1  = sqrt(self.p[\"sigma2_1\"])\n",
    "        mean2 = float(item.dot(self.p[\"phi_2\"]))\n",
    "        std2  = sqrt(self.p[\"sigma2_2\"])\n",
    "        \n",
    "        if np.random.rand() < self.p[\"w\"]:\n",
    "            return normal(mean1, std1), 1\n",
    "        else:\n",
    "            return normal(mean2, std2), 0\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        \n",
    "        return self.incompletelogl()\n",
    "        \n",
    "        ll         = zeros(20)\n",
    "        phi_1_diff = self.p[\"phi_1\"] - self.mu_phi0\n",
    "        phi_2_diff = self.p[\"phi_2\"] - self.mu_phi0\n",
    "        phi_1_err  = phi_1_diff.T.dot(phi_1_diff)\n",
    "        phi_2_err  = phi_2_diff.T.dot(phi_2_diff)\n",
    "        err_1      = (self.Y - self.X.dot(self.p[\"phi_1\"])) ** 2\n",
    "        err_2      = (self.Y - self.X.dot(self.p[\"phi_2\"])) ** 2\n",
    "        \n",
    "        gamma = self.gamma\n",
    "        \n",
    "        ### posterior factorizes p(y,z,w,phi,sigma) = p(y,z)p(w)p(phi)p(sigma)\n",
    "        #                                           = p(y)p(z)p(w)p(phi)p(sigma)\n",
    "        \n",
    "        ### p(y,z)\n",
    "        ll[0] =     gamma.dot(    self.p[\"w\"]  * norm.logpdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"])) )\n",
    "        ll[1] = (1-gamma).dot( (1-self.p[\"w\"]) * norm.logpdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"])) )\n",
    "        \n",
    "        ### p(z) already in p(y,z)\n",
    "#         ll[4] = np.sum((gamma * log(self.p[\"w\"])) + ((1 - gamma) * log(1 - self.p[\"w\"])))\n",
    "        \n",
    "        ### p(w)\n",
    "        ll[5] = self.w_gamma_ln_multiplier\n",
    "        ll[6] = (self.alpha_w0 - 1) * self.p[\"w\"]\n",
    "        ll[7] = (self.beta_w0  - 1) * (1 - self.p[\"w\"])\n",
    "\n",
    "        ### p(phi)\n",
    "        # phi_1\n",
    "        ll[8]  = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_1\"]) + log(self.lbd_phi0) )\n",
    "        ll[9]  = - 0.5 * phi_1_err / (self.lbd_phi0 * self.p[\"sigma2_1\"])\n",
    "        # phi_2\n",
    "        ll[10] = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_2\"]) + log(self.lbd_phi0) )\n",
    "        ll[11] = - 0.5 * phi_2_err / (self.lbd_phi0 * self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(sigma2)\n",
    "        # sigma2_1\n",
    "        ll[12] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[13] = - gammaln(self.alpha_s20)\n",
    "        ll[14] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_1\"])\n",
    "        ll[15] = - self.beta_s20 / self.p[\"sigma2_1\"]\n",
    "        # sigma2_2\n",
    "        ll[16] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[17] = - gammaln(self.alpha_s20)\n",
    "        ll[18] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_2\"])\n",
    "        ll[19] = - self.beta_s20 / self.p[\"sigma2_2\"]\n",
    "        \n",
    "        return np.sum(ll), ll\n",
    "\n",
    "\n",
    "    def incompletelogl(self):\n",
    "        \"\"\"\n",
    "            Calculates the incomplete data log likelihood for this model.\n",
    "            Returns the incomplete logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll         = zeros(20)\n",
    "        phi_1_diff = self.p[\"phi_1\"] - self.mu_phi0\n",
    "        phi_2_diff = self.p[\"phi_2\"] - self.mu_phi0\n",
    "        phi_1_err  = phi_1_diff.T.dot(phi_1_diff)\n",
    "        phi_2_err  = phi_2_diff.T.dot(phi_2_diff)\n",
    "        \n",
    "        ### p(y)\n",
    "        N1 = norm.pdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"]))\n",
    "        N2 = norm.pdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"]))\n",
    "        ll[0] = np.sum( np.log( self.p[\"w\"]*N1 + (1-self.p[\"w\"])*N2 ) )\n",
    "        \n",
    "        ### p(w)\n",
    "        ll[1] = self.w_gamma_ln_multiplier\n",
    "        ll[2] = (self.alpha_w0 - 1) * self.p[\"w\"]\n",
    "        ll[3] = (self.beta_w0  - 1) * (1 - self.p[\"w\"])\n",
    "\n",
    "        ### p(phi)\n",
    "        # phi_1\n",
    "        ll[4]  = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_1\"]) + log(self.lbd_phi0) )\n",
    "        ll[5]  = - 0.5 * phi_1_err / (self.lbd_phi0 * self.p[\"sigma2_1\"])\n",
    "        # phi_2\n",
    "        ll[6] = - 0.5 * ( self.pdata * log(2 * pi * self.p[\"sigma2_2\"]) + log(self.lbd_phi0) )\n",
    "        ll[7] = - 0.5 * phi_2_err / (self.lbd_phi0 * self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(sigma2)\n",
    "        # sigma2_1\n",
    "        ll[8] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[9] = - gammaln(self.alpha_s20)\n",
    "        ll[10] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_1\"])\n",
    "        ll[11] = - self.beta_s20 / self.p[\"sigma2_1\"]\n",
    "        # sigma2_2\n",
    "        ll[12] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[13] = - gammaln(self.alpha_s20)\n",
    "        ll[14] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_2\"])\n",
    "        ll[15] = - self.beta_s20 / self.p[\"sigma2_2\"]\n",
    "        \n",
    "        return np.sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        \n",
    "        # ==================== E-STEP ====================\n",
    "      \n",
    "        # norm.pdf works on a vector, returning probability for each separately\n",
    "        propto_gamma1 =      self.p[\"w\"]  * norm.pdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"]))\n",
    "        propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"]))\n",
    "\n",
    "        self.gamma = propto_gamma1 / (propto_gamma1 + propto_gamma2) # responsibilities\n",
    "\n",
    "        # ==================== M-STEP ====================\n",
    "\n",
    "        # ========== Component weights w ==========\n",
    "        num = 2*np.sum(self.gamma) + self.alpha_w0 - 1\n",
    "        den = 2*self.ndata + self.alpha_w0 + self.beta_w0 - 2\n",
    "        self.p[\"w\"] = num / den\n",
    "\n",
    "        self.assert_logl_increased(\"w\")\n",
    "    \n",
    "    \n",
    "        # ========== Variances sigma2 ==========\n",
    "        # phi_1 and phi_2 still have the previous value, i.e. from step s, we are calculating sigma for step s+1\n",
    "        \n",
    "        # sigma2_1\n",
    "        phie = np.sum((self.p[\"phi_1\"] - self.mu_phi0) ** 2)  / self.lbd_phi0\n",
    "        phiX = self.p[\"phi_1\"].dot(self.X.T)\n",
    "        target_err = (self.Y - phiX)**2\n",
    "        err = self.gamma.dot(target_err)\n",
    "        num = 2*self.beta_s20 + err + phie\n",
    "        den = 2*self.alpha_s20 + 2.0 + np.sum(self.gamma) + self.pdata\n",
    "        self.p[\"sigma2_1\"] = num / den\n",
    "        if self.p[\"sigma2_1\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_1 < 0.0\")\n",
    "        \n",
    "        # sigma2_2\n",
    "        phie = np.sum((self.p[\"phi_2\"] - self.mu_phi0) ** 2)  / self.lbd_phi0\n",
    "        phiX = self.p[\"phi_2\"].dot(self.X.T)\n",
    "        target_err = (self.Y - phiX)**2\n",
    "        err = (1-self.gamma).dot(target_err)\n",
    "        num = 2*self.beta_s20 + err + phie\n",
    "        den = 2*self.alpha_s20 + 2.0 + np.sum(1-self.gamma) + self.pdata\n",
    "        self.p[\"sigma2_2\"] = num / den\n",
    "        if self.p[\"sigma2_2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_2 < 0.0\")\n",
    "\n",
    "#         self.assert_logl_increased(\"sigma2 update\")\n",
    "        \n",
    "        \n",
    "        # ========== Variables phi ==========\n",
    "        \n",
    "        # phi_1\n",
    "        sum_gammayx = self.gamma.T.dot( (Y * self.X.T).T )\n",
    "        resp_matrix = eye(self.ndata) * self.gamma\n",
    "        sum_gammaxx = self.X.T.dot(resp_matrix.dot(self.X))\n",
    "        sigma_mu        = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv   = self.sigma_phi0_inv + sum_gammaxx\n",
    "        self.p[\"phi_1\"] = solve(sigma_phi_inv, sigma_mu + sum_gammayx)\n",
    "        \n",
    "        # phi_2\n",
    "        sum_gammayx = (1-self.gamma).T.dot(  (Y * self.X.T).T  )\n",
    "        resp_matrix = eye(self.ndata) * (1-self.gamma)\n",
    "        sum_gammaxx = self.X.T.dot(resp_matrix.dot(self.X))\n",
    "        sigma_mu        = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv   = self.sigma_phi0_inv + sum_gammaxx\n",
    "        self.p[\"phi_2\"] = solve(sigma_phi_inv, sigma_mu + sum_gammayx)\n",
    "\n",
    "        self.assert_logl_increased(\"phi update\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated 50 training data and 50 validation data from true model:\n",
      "sigma2_1 = 0.20084251761098823\n",
      "phi_1 = [-0.26205744  0.50160456  0.11878045]\n",
      "w = 0.49584751593617793\n",
      "phi_2 = [ 0.36369284 -0.53695191 -0.64470951]\n",
      "sigma2_2 = 0.12282568901201733\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get hyperparameters for model\n",
    "hyperp = get_hyperp()\n",
    "# generate 50 training data and 20 validation data locations of dim=1\n",
    "ndata = 50\n",
    "ndata_v = 50\n",
    "pdata = 3\n",
    "X = generate_X(ndata, pdata)\n",
    "X_v = generate_X(ndata_v, pdata)\n",
    "\n",
    "true_model = EM_algo_MM(hyperp, ndata=ndata, pdata=pdata)\n",
    "Y, Z = generate_YZ(X, true_model)\n",
    "Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "    (ndata, ndata_v))\n",
    "true_model.print_p()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model fit (logl -39.49) after 37 iterations (alim reached)\n",
      "\n",
      "MAP estimate of true model parameters:\n",
      "sigma2_1 = 0.195323439607\n",
      "phi_1 = [-0.23776231  0.42601827  0.10323428]\n",
      "w = 0.575181200737\n",
      "phi_2 = [ 0.50939278 -0.48942903 -0.59693865]\n",
      "sigma2_2 = 0.139330614719\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# generate a model for estimating the parameters of the\n",
    "# true model based on the observations (X, Y) we just made\n",
    "# model = EM_algo_MM(hyperp, X, Y)\n",
    "model = EM_algo_MM(hyperp, X, Y)\n",
    "i, logl, r = model.EM_fit()\n",
    "print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "        (logl, i, r))\n",
    "print(\"\")\n",
    "print(\"MAP estimate of true model parameters:\")\n",
    "model.print_p()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if possible, plot samples, true model and estimated model\n",
    "if pdata == 1:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "#         plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "#         print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "#                 true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "#         print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "#                 model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "\n",
    "    y = true_model.p[\"phi_1\"] * x\n",
    "    color = 'orange'\n",
    "    plt.plot(x, y, color, label=\"true1\")\n",
    "#     plt.fill_between(x, y + 1.96 * sqrt(true_model.p[\"sigma2_1\"]), y - 1.96 * sqrt(true_model.p[\"sigma2_1\"]), alpha=0.1, facecolor=color, interpolate=True)\n",
    "    \n",
    "    y = true_model.p[\"phi_2\"] * x\n",
    "    color = 'green'\n",
    "    plt.plot(x, y, color, label=\"true1\")\n",
    "#     plt.fill_between(x, y + 1.96 * sqrt(true_model.p[\"sigma2_2\"]), y - 1.96 * sqrt(true_model.p[\"sigma2_2\"]), alpha=0.1, facecolor=color, interpolate=True)\n",
    "\n",
    "    # Components\n",
    "    y = model.p[\"phi_1\"] * x\n",
    "    color = 'red'\n",
    "    plt.plot(x, y, color, label=\"component1\")\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(model.p[\"sigma2_1\"]), y - 1.96 * sqrt(model.p[\"sigma2_1\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "    y = model.p[\"phi_2\"] * x\n",
    "    color = 'blue'\n",
    "    plt.plot(x, y, color, label=\"component2\")\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(model.p[\"sigma2_2\"]), y - 1.96 * sqrt(model.p[\"sigma2_2\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sigma2_1': 0, 'phi_1': 0, 'w': 0, 'phi_2': 0, 'sigma2_2': 0}\n",
      "0 0 50\n",
      "{'sigma2_1': 0, 'phi_1': 0, 'w': 0, 'phi_2': 0, 'sigma2_2': 0}\n",
      "0.33065527303\n",
      "0.0210790470669\n"
     ]
    }
   ],
   "source": [
    "mm_cumulative_error = 0.0\n",
    "mm_max_diff = 0.0\n",
    "foodict = {'w': 0, 'phi_1': 0, 'phi_2': 0, 'sigma2_1': 0, 'sigma2_2': 0}\n",
    "bardict = {'w': 0, 'phi_1': 0, 'phi_2': 0, 'sigma2_1': 0, 'sigma2_2': 0}\n",
    "counter = 0\n",
    "counterOther = 0\n",
    "counterAlim = 0\n",
    "for truru in range(50):\n",
    "    try:\n",
    "        # generate a model for estimating the parameters of the\n",
    "        # true model based on the observations (X, Y) we just made\n",
    "        model = EM_algo_MM(hyperp, X, Y)\n",
    "        i, logl, r = model.EM_fit()\n",
    "        if r == \"alim\":\n",
    "            counterAlim += 1\n",
    "    except ValueError as e:\n",
    "        counter = counter + 1\n",
    "        foodict[str(e)] = foodict[str(e)] + 1\n",
    "    except RuntimeError as e:\n",
    "        counterOther = counterOther + 1\n",
    "        bardict[str(e)] = bardict[str(e)] + 1\n",
    "\n",
    "print(foodict)\n",
    "print(counter, counterOther, counterAlim)\n",
    "print(bardict)\n",
    "print(mm_cumulative_error)\n",
    "print(mm_max_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P=1 2*w check each\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 191 809\n",
    "# {'phi_2': 47, 'w': 0, 'phi_1': 39, 'sigma2_2': 104, 'sigma2_1': 1}\n",
    "# 97.4147580405\n",
    "# 3.59291931674\n",
    "# P=1 w check each\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 200 800\n",
    "# {'phi_2': 71, 'w': 4, 'phi_1': 32, 'sigma2_2': 93, 'sigma2_1': 0}\n",
    "# 92.1741867772\n",
    "# 2.57140877529\n",
    "# P=1 w check w and after\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 2 998\n",
    "# {'phi_2': 0, 'w': 2, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 12.8199038625\n",
    "# 0.191820591439\n",
    "# P=1 2*w check w and after\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 2.98402674426\n",
    "# 0.0398580346214\n",
    "# prev with update to phi logl calc formula...\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 2.84134098953\n",
    "# 0.0422773055005\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 5.68757981874\n",
    "# 0.0336705559807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# P=25 2*w check?\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 3.08312064566\n",
    "# 0.00761159372594\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 0 0 1000\n",
    "# {'phi_2': 0, 'w': 0, 'phi_1': 0, 'sigma2_2': 0, 'sigma2_1': 0}\n",
    "# 3.37453377128\n",
    "# 0.00951989469854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting program\n",
      "\n",
      "Generated 20 training data and 50 validation data from true model:\n",
      "phi    : (-0.84, 0.20, -1.44)\n",
      "sigma2 : 0.338\n",
      "\n",
      "Model fit (logl -21.77) after 2 iterations (alim reached)\n",
      "\n",
      "MAP estimate of true model parameters:\n",
      "phi    : (-0.62, 0.25, -1.30)\n",
      "sigma2 : 0.318\n",
      "\n",
      "Crossvalidated logl: -55.81\n"
     ]
    }
   ],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        Executed when program is run.\n",
    "    \"\"\"\n",
    "    print(\"Starting program\")\n",
    "    print(\"\")\n",
    "    test_LM_model()\n",
    "\n",
    "\n",
    "def test_LM_model():\n",
    "    \"\"\"\n",
    "        Example that demonstrates how to call the model.\n",
    "    \"\"\"\n",
    "    # get hyperparameters for model\n",
    "    hyperp = get_hyperp()\n",
    "    # generate 50 training data and 20 validation data locations of dim=1\n",
    "    ndata = 20\n",
    "    ndata_v = 50\n",
    "    pdata = 3\n",
    "    X = generate_X(ndata, pdata)\n",
    "    X_v = generate_X(ndata_v, pdata)\n",
    "    # intialize true model randomly and draw observations from it\n",
    "    true_model = EM_algo_LM(hyperp, ndata=ndata, pdata=pdata)\n",
    "    Y, Z = generate_YZ(X, true_model)\n",
    "    Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "    print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "            (ndata, ndata_v))\n",
    "    true_model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # generate a model for estimating the parameters of the\n",
    "    # true model based on the observations (X, Y) we just made\n",
    "    model = EM_algo_LM(hyperp, X, Y)\n",
    "    i, logl, r = model.EM_fit()\n",
    "    print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "            (logl, i, r))\n",
    "    print(\"\")\n",
    "    print(\"MAP estimate of true model parameters:\")\n",
    "    model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # crossvalidate the estimated model with the validation data\n",
    "    fit_params = model.get_p()\n",
    "    model_v = EM_algo_LM(hyperp, X_v, Y_v)\n",
    "    model_v.set_p(fit_params)\n",
    "    logl, ll = model_v.logl()\n",
    "    print(\"Crossvalidated logl: %.2f\" % (logl))\n",
    "\n",
    "    # if possible, plot samples, true model and estimated model\n",
    "    if pdata != 1:\n",
    "        return\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "    plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "    print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "            true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "    print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "            model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def print_linear_model(x, phi, sigma2, color, label):\n",
    "    \"\"\"\n",
    "        Print linear model mean and 95% confidence interval.\n",
    "    \"\"\"\n",
    "    y = phi * x\n",
    "    plt.plot(x, y, color, label=label)\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(sigma2), y - 1.96 * sqrt(sigma2), \\\n",
    "            alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1195.078125px",
   "right": "20px",
   "top": "120px",
   "width": "186px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
