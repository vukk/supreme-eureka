{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import outer, eye, ones, zeros, diag, log, sqrt, exp, pi\n",
    "from numpy.linalg import inv, solve\n",
    "from numpy.random import multivariate_normal as mvnormal, normal, gamma, beta, binomial\n",
    "from scipy.special import gammaln\n",
    "from scipy.stats import norm, multivariate_normal\n",
    "\n",
    "from numpy import zeros\n",
    "from numpy.random import randn\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import arange, min, max, sqrt, mean, std\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM-algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     6
    ],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "class EM_algo():\n",
    "    \"\"\"\n",
    "        A superclass for different EM-fitted models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, hyperparams, X=None, Y=None, ndata=0, pdata=0):\n",
    "        \"\"\"\n",
    "            Initialize model based either on given data (X, Y) or\n",
    "            on given data dimensionality (ndata, pdata).\n",
    "        \"\"\"\n",
    "        if not X is None and not Y is None:\n",
    "            self.X = X\n",
    "            self.Y = Y\n",
    "            self.ndata = len(self.X)\n",
    "            self.pdata = len(self.X[0])\n",
    "        if ndata and pdata:\n",
    "            self.X = None\n",
    "            self.Y = None\n",
    "            self.ndata = ndata\n",
    "            self.pdata = pdata\n",
    "        self.h = hyperparams\n",
    "        self.p = dict() # model parameters\n",
    "        self.reset()\n",
    "        if not X is None and not Y is None:\n",
    "            self.current_logl, self.cll = self.logl()\n",
    "\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the drawn y and z-values.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError(\"Subclass implements\")\n",
    "\n",
    "\n",
    "    def EM_fit(self, alim=1e-10, maxit=1e4):\n",
    "        \"\"\"\n",
    "            Calls the EM_iter repeatedly until the log likelihood\n",
    "            of the model increases less than 'alim' in absolute\n",
    "            value or after 'maxit' iterations have been done.\n",
    "\n",
    "            Returns the number of EM-iterations, final log likelihood\n",
    "            value and a string that explains the end condition.\n",
    "        \"\"\"\n",
    "        logl, ll = self.logl()\n",
    "        for i in range(int(maxit)):\n",
    "            self.EM_iter()\n",
    "            logl2, ll2 = self.logl()\n",
    "            adiff = abs(logl2 - logl)\n",
    "            if adiff < alim:\n",
    "                return i+1, logl2, \"alim\"\n",
    "            logl = logl2\n",
    "        return maxit, logl2, \"maxit\"\n",
    "\n",
    "\n",
    "    def assert_logl_increased(self, event):\n",
    "        \"\"\"\n",
    "            Checks that the log likelihood increased since model\n",
    "            initialization or the time this function was last called.\n",
    "        \"\"\"\n",
    "        newlogl, ll = self.logl()\n",
    "#         if self.current_logl - newlogl > 1e-3:\n",
    "        if self.current_logl - newlogl > 0.1:\n",
    "            self.debug_logl(self.cll, ll)\n",
    "            raise ValueError(\"logl decreased after %s\" % (event))\n",
    "        self.current_logl, self.cll = newlogl, ll\n",
    "\n",
    "\n",
    "    def get_p(self):\n",
    "        \"\"\"\n",
    "            Returns a copy of the model parameters.\n",
    "        \"\"\"\n",
    "        return copy.deepcopy(self.p)\n",
    "\n",
    "\n",
    "    def set_p(self, p):\n",
    "        \"\"\"\n",
    "            Sets the model parameters.\n",
    "        \"\"\"\n",
    "        self.p = p.copy()\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        for k, v in self.p.items():\n",
    "            print(\"%s = %s\" % (k, v))\n",
    "\n",
    "\n",
    "    def pretty_vector(self, x):\n",
    "        \"\"\"\n",
    "            Returns a formatted version of a vector.\n",
    "        \"\"\"\n",
    "        s = [\"(\"]\n",
    "        s.extend([\"%.2f, \" % (xi) for xi in x[:-1]])\n",
    "        s.append(\"%.2f)\" % (x[-1]))\n",
    "        return \"\".join(s)\n",
    "\n",
    "\n",
    "    def debug_logl(self, ll1, ll2):\n",
    "        \"\"\"\n",
    "            Prints an analysis of the per-term change in\n",
    "            log likelihood from ll1 to ll2.\n",
    "        \"\"\"\n",
    "        print(\"Logl      before     after\")\n",
    "        for v1, v2, i in zip(ll1, ll2, range(len(ll1))):\n",
    "            if v1 > v2:\n",
    "                d = \">\"\n",
    "            elif v2 > v1:\n",
    "                d = \"<\"\n",
    "            else:\n",
    "                d = \"=\"\n",
    "            print(\"Term %02d: %7.3f %s %7.3f\" % (i, v1, d, v2))\n",
    "        print(\"Total    %7.3f   %7.3f\" % (sum(ll1), sum(ll2)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "class EM_algo_LM(EM_algo):\n",
    "    \"\"\"\n",
    "        A linear gaussian model.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "\n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p           = dict()\n",
    "        self.p[\"sigma2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2\"] * self.sigma_phi0)\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value (and a constant z-value for compatibility)\n",
    "        \"\"\"\n",
    "        mean = float(item.dot(self.p[\"phi\"]))\n",
    "        std  = sqrt(self.p[\"sigma2\"])\n",
    "        return normal(mean, std), 1\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll    = zeros(8)\n",
    "        phie  = self.p[\"phi\"] - self.mu_phi0\n",
    "        err   = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        # p(y)\n",
    "        ll[0] = - 0.5 * log(2 * pi * self.p[\"sigma2\"]) * self.ndata\n",
    "        ll[1] = sum(- 0.5 * err / self.p[\"sigma2\"])\n",
    "        # p(phi)\n",
    "        ll[2] = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2\"]) * self.pdata\n",
    "        ll[3] = - 0.5 * phie.T.dot(phie) / (self.lbd_phi0 * self.p[\"sigma2\"])\n",
    "        # p(sigma2)\n",
    "        ll[4] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[5] = - gammaln(self.alpha_s20)\n",
    "        ll[6] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2\"])\n",
    "        ll[7] = - self.beta_s20 / self.p[\"sigma2\"]\n",
    "        return sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        # phi\n",
    "        sumxx         = self.X.T.dot(self.X)\n",
    "        sumxy         = self.X.T.dot(self.Y)\n",
    "        sigma_mu      = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv = self.sigma_phi0_inv + sumxx\n",
    "#         print(\"sigma_phi_inv.shape\", sigma_phi_inv.shape)\n",
    "        self.p[\"phi\"] = solve(sigma_phi_inv, sigma_mu + sumxy)\n",
    "        print(\"sumxx\", type(sumxx), \"sumxy\", type(sumxy), \"sigma_mu\", type(sigma_mu), \"sigma_phi_inv\", type(sigma_phi_inv))\n",
    "        print(\"sumxx\", sumxx.shape, \"sumxy\", sumxy.shape, \"sigma_mu\", sigma_mu.shape, \"sigma_phi_inv\", sigma_phi_inv.shape)\n",
    "        self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "        # sigma2\n",
    "        phie = (self.p[\"phi\"] - self.mu_phi0) ** 2\n",
    "        err  = (self.X.dot(self.p[\"phi\"]) - self.Y) ** 2\n",
    "        num  = self.beta_s20 + 0.5 * sum(err) + 0.5 * sum(phie) / self.lbd_phi0\n",
    "        den  = self.alpha_s20 + 1.0 + 0.5 * (self.ndata + self.pdata)\n",
    "        self.p[\"sigma2\"] = num / den\n",
    "        print(\"phie\", type(phie), \"err\", type(err), \"num\", type(num), \"den\", type(den))\n",
    "        print(\"phie\", phie.shape, \"err\", err.shape, \"num\", num.shape, \"den\", \"is a float\")\n",
    "        if self.p[\"sigma2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2 < 0.0\")\n",
    "        self.assert_logl_increased(\"sigma2 update\")\n",
    "\n",
    "\n",
    "    def print_p(self):\n",
    "        \"\"\"\n",
    "            Prints the model parameters, one at each line.\n",
    "        \"\"\"\n",
    "        print(\"phi    : %s\" % (self.pretty_vector(self.p[\"phi\"])))\n",
    "        print(\"sigma2 : %.3f\" % (self.p[\"sigma2\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "def generate_X(ndata, pdata):\n",
    "    \"\"\"\n",
    "        Return a matrix of normally distributed random values.\n",
    "    \"\"\"\n",
    "    X = randn(ndata, pdata)\n",
    "    return X\n",
    "\n",
    "\n",
    "def generate_YZ(X, distribution):\n",
    "    \"\"\"\n",
    "        Draw observations Y and latent variable values Z from a distribution.\n",
    "    \"\"\"\n",
    "    ndata = len(X)\n",
    "    Y = zeros(ndata)\n",
    "    Z = zeros(ndata)\n",
    "    for i in range(ndata):\n",
    "        Y[i], Z[i] = distribution.draw(X[i])\n",
    "    return Y, Z\n",
    "\n",
    "\n",
    "def get_hyperp():\n",
    "    \"\"\"\n",
    "        Return model hyperparameters.\n",
    "    \"\"\"\n",
    "    return {\n",
    "            \"alpha_s20\": 5.0,\n",
    "            \"beta_s20\" : 1.0,\n",
    "            \"lbd_phi0\" : 1.0,\n",
    "            \"mu_phi0\"  : 0.0,\n",
    "            \"alpha_w0\" : 3.0,\n",
    "            \"beta_w0\"  : 3.0,\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixture model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class EM_algo_MM(EM_algo):\n",
    "    \"\"\"\n",
    "        A mixture of two linear models.\n",
    "    \"\"\"\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "            Reset priors and draw parameter estimates from prior.\n",
    "        \"\"\"\n",
    "        # priors\n",
    "        self.alpha_w0       = self.h[\"alpha_w0\"]\n",
    "        self.beta_w0        = self.h[\"beta_w0\"]\n",
    "\n",
    "        # Same priors for phi1 and phi2, s2_1, s2_2, don't bother to copy vars twice\n",
    "        # i.e. alpha_s2_1_0 = alpha_s2_2_0 = alpha_s20\n",
    "        self.lbd_phi0       = self.h[\"lbd_phi0\"]\n",
    "        self.alpha_s20      = self.h[\"alpha_s20\"]\n",
    "        self.beta_s20       = self.h[\"beta_s20\"]\n",
    "        self.sigma_phi0     = eye(self.pdata) * self.h[\"lbd_phi0\"]\n",
    "        self.sigma_phi0_inv = eye(self.pdata) / self.h[\"lbd_phi0\"]\n",
    "        self.mu_phi0        = ones(self.pdata) * self.h[\"mu_phi0\"]\n",
    "        \n",
    "        # Precalculations:\n",
    "        self.w_gamma_ln_multiplier  = gammaln(self.alpha_w0 + self.beta_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.alpha_w0)\n",
    "        self.w_gamma_ln_multiplier -= gammaln(self.beta_w0)\n",
    "        \n",
    "        \n",
    "        # initial parameter estimates drawn from prior\n",
    "        self.p             = dict()\n",
    "        # Weights\n",
    "        self.p[\"w\"]        = beta(self.alpha_w0, self.beta_w0)\n",
    "        # Responsibilities (TODO: do we need this here?)\n",
    "        self.p[\"gamma\"]    = binomial(1, self.p[\"w\"], self.ndata)\n",
    "        # Component 1\n",
    "        self.p[\"sigma2_1\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_1\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_1\"] * self.sigma_phi0)\n",
    "        # Component 2\n",
    "        self.p[\"sigma2_2\"] = 1.0 / gamma(self.alpha_s20, 1.0 / self.beta_s20) # inverse gamma\n",
    "        self.p[\"phi_2\"]    = mvnormal(self.mu_phi0, self.p[\"sigma2_2\"] * self.sigma_phi0)\n",
    "        \n",
    "        print(\"START w\", self.p[\"w\"])\n",
    "        print(\"START phi_1\", self.p[\"phi_1\"])\n",
    "        print(\"START sigma2_1\", self.p[\"sigma2_1\"])\n",
    "        print(\"START phi_2\", self.p[\"phi_2\"])\n",
    "        print(\"START sigma2_2\", self.p[\"sigma2_2\"])\n",
    "\n",
    "\n",
    "    def draw(self, item):\n",
    "        \"\"\"\n",
    "            Draw a data sample from the current predictive distribution.\n",
    "            Returns the y-value and z-value\n",
    "        \"\"\"\n",
    "        mean1 = float(item.dot(self.p[\"phi_1\"]))\n",
    "        std1  = sqrt(self.p[\"sigma2_1\"])\n",
    "        mean2 = float(item.dot(self.p[\"phi_2\"]))\n",
    "        std2  = sqrt(self.p[\"sigma2_2\"])\n",
    "        \n",
    "        if np.random.rand() < self.p[\"w\"]:\n",
    "            return normal(mean1, std1), 1\n",
    "        else:\n",
    "            return normal(mean2, std2), 2\n",
    "\n",
    "\n",
    "    def logl(self):\n",
    "        \"\"\"\n",
    "            Calculates the full log likelihood for this model.\n",
    "            Returns the logl (and the values of each term for debugging purposes)\n",
    "        \"\"\"\n",
    "        ll         = zeros(20)\n",
    "        phi_1_diff = self.p[\"phi_1\"] - self.mu_phi0\n",
    "        phi_2_diff = self.p[\"phi_2\"] - self.mu_phi0\n",
    "        phi_1_err  = phi_1_diff.T.dot(phi_1_diff)\n",
    "        phi_2_err  = phi_2_diff.T.dot(phi_2_diff)\n",
    "        err_1      = (self.X.dot(self.p[\"phi_1\"]) - self.Y) ** 2\n",
    "        err_2      = (self.X.dot(self.p[\"phi_2\"]) - self.Y) ** 2\n",
    "        \n",
    "        # Responsibilities\n",
    "#         propto_gamma1 =      self.p[\"w\"]  * norm.pdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"]))\n",
    "#         propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"]))\n",
    "#         gamma = propto_gamma1 / (propto_gamma1 + propto_gamma2)\n",
    "        gamma = self.p[\"gamma\"]\n",
    "        \n",
    "        ### posterior factorizes p(y,z,w,phi,sigma) = p(y)p(z)p(w)p(phi)p(sigma)\n",
    "        \n",
    "        ### p(y)\n",
    "#         ll[0] = self.p[\"w\"]       * (-0.5 * log(2 * pi * self.p[\"sigma2_1\"]) * self.ndata)\n",
    "#         ll[1] = self.p[\"w\"]       * np.sum(- 0.5 * err_1 / self.p[\"sigma2_1\"])\n",
    "#         ll[2] = (1 - self.p[\"w\"]) * (-0.5 * log(2 * pi * self.p[\"sigma2_2\"]) * self.ndata)\n",
    "#         ll[3] = (1 - self.p[\"w\"]) * np.sum(- 0.5 * err_2 / self.p[\"sigma2_2\"])\n",
    "\n",
    "#         ll[0] =       gamma.dot(-0.5 * log(2 * pi * self.p[\"sigma2_1\"]) -0.5 * err_1 / self.p[\"sigma2_1\"])\n",
    "#         ll[1] = 0\n",
    "#         ll[2] = (1 - gamma).dot(-0.5 * log(2 * pi * self.p[\"sigma2_2\"]) -0.5 * err_2 / self.p[\"sigma2_2\"])\n",
    "#         ll[3] = 0\n",
    "\n",
    "        ll[0] = -0.5 * log(2 * pi * self.p[\"sigma2_1\"]) * np.sum(gamma)\n",
    "        ll[1] = gamma.dot(-0.5 * err_1 / self.p[\"sigma2_1\"])\n",
    "        ll[2] = -0.5 * log(2 * pi * self.p[\"sigma2_2\"]) * np.sum(1 - gamma)\n",
    "        ll[3] = (1 - gamma).dot(-0.5 * err_2 / self.p[\"sigma2_2\"])\n",
    " \n",
    "        \n",
    "        ### p(z)\n",
    "        ll[4] = np.sum((gamma * log(self.p[\"w\"])) + ((1 - gamma) * log(1 - self.p[\"w\"])))\n",
    "        \n",
    "        ### p(w)\n",
    "        ll[5] = self.w_gamma_ln_multiplier\n",
    "        ll[6] = (self.alpha_w0 - 1) * self.p[\"w\"]\n",
    "        ll[7] = (self.beta_w0  - 1) * (1 - self.p[\"w\"])\n",
    "        \n",
    "        ### p(phi)\n",
    "        # phi_1\n",
    "        ll[8]  = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2_1\"]) * self.pdata\n",
    "        ll[9]  = - 0.5 * phi_1_err / (self.lbd_phi0 * self.p[\"sigma2_1\"])\n",
    "        # phi_2\n",
    "        ll[10] = - 0.5 * log(2 * pi * self.lbd_phi0 * self.p[\"sigma2_2\"]) * self.pdata\n",
    "        ll[11] = - 0.5 * phi_2_err / (self.lbd_phi0 * self.p[\"sigma2_2\"])\n",
    "        \n",
    "        ### p(sigma2)\n",
    "        # sigma2_1\n",
    "        ll[12] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[13] = - gammaln(self.alpha_s20)\n",
    "        ll[14] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_1\"])\n",
    "        ll[15] = - self.beta_s20 / self.p[\"sigma2_1\"]\n",
    "        # sigma2_2\n",
    "        ll[16] = self.alpha_s20 * log(self.beta_s20)\n",
    "        ll[17] = - gammaln(self.alpha_s20)\n",
    "        ll[18] = - (self.alpha_s20 + 1.0) * log(self.p[\"sigma2_2\"])\n",
    "        ll[19] = - self.beta_s20 / self.p[\"sigma2_2\"]\n",
    "        \n",
    "        return np.sum(ll), ll\n",
    "\n",
    "\n",
    "    def EM_iter(self):\n",
    "        \"\"\"\n",
    "            Executes a single round of EM updates for this model.\n",
    "\n",
    "            Has checks to make sure that updates increase logl and\n",
    "            that parameter values stay in sensible limits.\n",
    "        \"\"\"\n",
    "        \n",
    "        Y = self.Y.view()\n",
    "        Y.shape = (self.ndata, 1) # set Y view shape\n",
    "\n",
    "        # ==================== E-STEP ====================\n",
    "\n",
    "        # ========== Responsibilities gamma ==========        \n",
    "        # norm.pdf works on a vector, returning probability for each separately\n",
    "        propto_gamma1 =      self.p[\"w\"]  * norm.pdf(self.Y, self.X.dot(self.p[\"phi_1\"]), sqrt(self.p[\"sigma2_1\"]))\n",
    "        propto_gamma2 = (1 - self.p[\"w\"]) * norm.pdf(self.Y, self.X.dot(self.p[\"phi_2\"]), sqrt(self.p[\"sigma2_2\"]))\n",
    "        \n",
    "        # elementwise, works because numpy\n",
    "        gamma = propto_gamma1 / (propto_gamma1 + propto_gamma2)\n",
    "        gammaView = gamma.view()\n",
    "        gammaView.shape = (1, self.ndata)\n",
    "#         print(\"gammaView.shape\", gammaView.shape)\n",
    "        self.p[\"gamma\"] = gamma\n",
    "        sum_gamma = np.sum(gamma)\n",
    "#         print(\"sum gamma\", sum_gamma, \"sum gamma+(1-gamma)\", np.sum(gamma+(1-gamma)))\n",
    "        \n",
    "\n",
    "        # ==================== M-STEP ====================\n",
    "\n",
    "        # ========== Component weights w ==========\n",
    "        num = sum_gamma + self.alpha_w0 - 1\n",
    "        den = self.ndata + self.alpha_w0 + self.beta_w0 - 2\n",
    "        self.p[\"w\"] = num / den\n",
    "        \n",
    "#         print(\"W LL DEBUG\")\n",
    "#         newlogl, ll = self.logl()\n",
    "#         self.debug_logl(self.cll, ll)\n",
    "#         self.assert_logl_increased(\"w update\")\n",
    "        \n",
    "        \n",
    "        # ========== Variables phi ==========\n",
    "        \n",
    "        # phi_1\n",
    "        sum_gammayx = gamma.T.dot(Y * self.X)\n",
    "        # NEW\n",
    "        sum_gammaxx = np.dot( X.T, (gammaView.T * X) )\n",
    "#         sum_gammaxx = np.dot( X.T, (gammarep.T * X) )\n",
    "        \n",
    "#         sum_gammaxx = np.zeros([self.pdata,self.pdata])\n",
    "#         print(\"gamma\", gamma.shape)\n",
    "#         for t in range(0,self.ndata):\n",
    "#             sum_gammaxx += gamma[t] * np.dot(X[t].T, X[t])\n",
    "# #         sum_gammaxx = gamma.T.dot((self.X * self.X))\n",
    "        # END NEW\n",
    "        sigma_phi_inv = self.sigma_phi0_inv + sum_gammaxx\n",
    "        print(\"self.sigma_phi0_inv\", self.sigma_phi0_inv.shape, \"sigma_phi_inv\", sigma_phi_inv.shape)\n",
    "#         print(\"sum_gammayx, sum_gammaxx SHAPES\", sum_gammayx.shape, sum_gammaxx.shape)\n",
    "        sigma_mu        = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv   = self.sigma_phi0_inv + sum_gammaxx\n",
    "        self.p[\"phi_1\"] = solve(sigma_phi_inv, sigma_mu + sum_gammayx)\n",
    "        print(\"sum_gammaxx\", sum_gammaxx.shape, \"sum_gammaxy\", sum_gammayx.shape, \"sigma_mu\", sigma_mu.shape, \"sigma_phi_inv\", sigma_phi_inv.shape)\n",
    "        print(\"phi_1\", self.p[\"phi_1\"])\n",
    "        \n",
    "        \n",
    "        # phi_2\n",
    "        sum_gammayx = (1-gamma).T.dot(Y * self.X)\n",
    "        # NEW\n",
    "        sum_gammaxx = np.dot( X.T, ((1-gammaView).T * X) )\n",
    "#         sum_gammaxx = np.dot( X.T, (one_minus_gammarep.T * X) )\n",
    "        \n",
    "#         sum_gammaxx = np.zeros([self.pdata,self.pdata])\n",
    "#         for t in range(0,self.ndata):\n",
    "#             sum_gammaxx += (1-gamma[t]) * np.dot(X[t].T, X[t])\n",
    "#         # END NEW\n",
    "# #         sum_gammaxx = (1-gamma).T.dot((self.X * self.X))\n",
    "        print(\"self.sigma_phi0_inv\", self.sigma_phi0_inv.shape, \"sigma_phi_inv\", sigma_phi_inv.shape)\n",
    "#         print(\"sum_gammayx, sum_gammaxx SHAPES\", sum_gammayx.shape, sum_gammaxx.shape)\n",
    "        sigma_mu        = self.sigma_phi0_inv.dot(self.mu_phi0)\n",
    "        sigma_phi_inv   = self.sigma_phi0_inv + sum_gammaxx\n",
    "        self.p[\"phi_2\"] = solve(sigma_phi_inv, sigma_mu + sum_gammayx)\n",
    "        print(\"sum_gammaxx\", sum_gammaxx.shape, \"sum_gammaxy\", sum_gammayx.shape, \"sigma_mu\", sigma_mu.shape, \"sigma_phi_inv\", sigma_phi_inv.shape)\n",
    "        print(\"phi_2\", self.p[\"phi_2\"])\n",
    "        \n",
    "#         print(\"PHI LL DEBUG\")\n",
    "#         newlogl, ll = self.logl()\n",
    "#         self.debug_logl(self.cll, ll)\n",
    "#         self.assert_logl_increased(\"phi update\")\n",
    "\n",
    "\n",
    "        # ========== Variances sigma2 ==========\n",
    "        \n",
    "        # sigma2_1\n",
    "        phie = np.sum((self.p[\"phi_1\"] - self.mu_phi0) ** 2)  / self.lbd_phi0\n",
    "        phiX = self.p[\"phi_1\"].dot(self.X.T)\n",
    "        target_err = (self.Y - phiX)**2\n",
    "        err_test = (self.X.dot(self.p[\"phi_1\"]) - self.Y) ** 2\n",
    "        print(\"err_test\", err_test.shape, type(err_test))\n",
    "        tsup = gamma.dot(err_test)\n",
    "        err = gamma.dot(target_err)\n",
    "        print(\"tsup-err\", tsup-err)\n",
    "        num = 2*self.beta_s20 + err + phie\n",
    "        den = 2*self.alpha_s20 + 2.0 + np.sum(gamma) + self.pdata\n",
    "        self.p[\"sigma2_1\"] = num / den\n",
    "        print(\"phie\", phie.shape, \"phiX.shape\", phiX.shape, \"target_err\", target_err.shape, \"err\", err.shape, \"num\", num.shape, \"den\", den.shape)\n",
    "        if self.p[\"sigma2_1\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_1 < 0.0\")\n",
    "        \n",
    "        # compare ratios\n",
    "        print(\"phie/P\", phie/self.pdata, \"phie\", phie, \"err/sumGamma\", err/np.sum(gamma), \"err\", err)\n",
    "        \n",
    "        \n",
    "        # sigma2_2\n",
    "        phie = np.sum((self.p[\"phi_2\"] - self.mu_phi0) ** 2)  / self.lbd_phi0\n",
    "        phiX = self.p[\"phi_2\"].dot(self.X.T)\n",
    "        target_err = (self.Y - phiX)**2\n",
    "        err = (1-gamma).dot(target_err)\n",
    "        num = 2*self.beta_s20 + err + phie\n",
    "        den = 2*self.alpha_s20 + 2.0 + np.sum(1-gamma) + self.pdata\n",
    "        self.p[\"sigma2_2\"] = num / den\n",
    "        print(\"phie\", phie.shape, \"phiX.shape\", phiX.shape, \"target_err\", target_err.shape, \"err\", err.shape, \"num\", num.shape, \"den\", den.shape)\n",
    "        if self.p[\"sigma2_2\"] < 0.0:\n",
    "            raise ValueError(\"sigma2_2 < 0.0\")\n",
    "        \n",
    "        # compare ratios\n",
    "        print(\"phie/P\", phie/self.pdata, \"phie\", phie, \"err/sum1-Gamma\", err/np.sum(1-gamma), \"err\", err)\n",
    "        \n",
    "        \n",
    "#         print(\"SIGMA LL DEBUG\")\n",
    "#         newlogl, ll = self.logl()\n",
    "#         self.debug_logl(self.cll, ll)\n",
    "        \n",
    "#         # if possible, plot samples, true model and estimated model\n",
    "#         if self.pdata == 1:\n",
    "#             plt.figure(figsize=(20,10))\n",
    "#             plt.scatter(self.X, self.Y, s=20, c='black', label=\"Training data\")\n",
    "#     #         plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "#             x = arange(min(self.X)-0.1, max(self.X)+0.1, 0.1)\n",
    "#     #         print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "#     #                 true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "#     #         print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "#     #                 model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "#             y = self.p[\"phi_1\"] * x\n",
    "#             color = 'red'\n",
    "#             plt.plot(x, y, color, label=\"component1\")\n",
    "#             plt.fill_between(x, y + 1.96 * sqrt(self.p[\"sigma2_1\"]), y - 1.96 * sqrt(self.p[\"sigma2_1\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "            \n",
    "#             y = self.p[\"phi_2\"] * x\n",
    "#             color = 'blue'\n",
    "#             plt.plot(x, y, color, label=\"component2\")\n",
    "#             plt.fill_between(x, y + 1.96 * sqrt(self.p[\"sigma2_2\"]), y - 1.96 * sqrt(self.p[\"sigma2_2\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "#             plt.legend(loc=1)\n",
    "#             plt.xlim(min(x), max(x))\n",
    "#             plt.xlabel(\"x\")\n",
    "#             plt.ylabel(\"y\")\n",
    "#             plt.show()\n",
    "#             input(\"Press Enter to continue...\")\n",
    "        \n",
    "        print(\"w\", self.p[\"w\"], \"phi_1\", self.p[\"phi_1\"], \"phi_2\", self.p[\"phi_2\"], \"s2_1\", self.p[\"sigma2_1\"], \"s2_2\", self.p[\"sigma2_2\"])\n",
    "        \n",
    "        self.assert_logl_increased(\"sigma2 update\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START w 0.25092333255459515\n",
      "START phi_1 [-0.27081164  0.13296189]\n",
      "START sigma2_1 0.13098674923551312\n",
      "START phi_2 [ 0.10960413 -0.03979587]\n",
      "START sigma2_2 0.1260899189083818\n",
      "Generated 200 training data and 50 validation data from true model:\n",
      "gamma = [1 1 0 0 1 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 1 0 0 0 0 1 0 1 0\n",
      " 1 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0\n",
      " 0 1 0 0 0 0 0 1 1 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0\n",
      " 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0]\n",
      "sigma2_1 = 0.13098674923551312\n",
      "sigma2_2 = 0.1260899189083818\n",
      "w = 0.25092333255459515\n",
      "phi_2 = [ 0.10960413 -0.03979587]\n",
      "phi_1 = [-0.27081164  0.13296189]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get hyperparameters for model\n",
    "hyperp = get_hyperp()\n",
    "# generate 50 training data and 20 validation data locations of dim=1\n",
    "ndata = 200\n",
    "ndata_v = 50\n",
    "pdata = 2\n",
    "X = generate_X(ndata, pdata)\n",
    "X_v = generate_X(ndata_v, pdata)\n",
    "\n",
    "true_model = EM_algo_MM(hyperp, ndata=ndata, pdata=pdata)\n",
    "Y, Z = generate_YZ(X, true_model)\n",
    "Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "    (ndata, ndata_v))\n",
    "true_model.print_p()\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "START w 0.8487573924735687\n",
      "START phi_1 [-0.07081325 -0.13479411]\n",
      "START sigma2_1 0.20828230605772718\n",
      "START phi_2 [ 0.26729434 -0.26509008]\n",
      "START sigma2_2 0.13372738808935772\n",
      "gammaView.shape (1, 200)\n",
      "self.sigma_phi0_inv (2, 2) sigma_phi_inv (2, 2)\n",
      "sum_gammayx, sum_gammaxx SHAPES (2,) (2, 2)\n",
      "sum_gammaxx (2, 2) sum_gammaxy (2,) sigma_mu (2,) sigma_phi_inv (2, 2)\n",
      "phi_1 [-0.03364863  0.00365736]\n",
      "self.sigma_phi0_inv (2, 2) sigma_phi_inv (2, 2)\n",
      "sum_gammayx, sum_gammaxx SHAPES (2,) (2, 2)\n",
      "sum_gammaxx (2, 2) sum_gammaxy (2,) sigma_mu (2,) sigma_phi_inv (2, 2)\n",
      "phi_2 [ 0.23182242 -0.13230175]\n",
      "err_test (200,) <class 'numpy.ndarray'>\n",
      "tsup-err 0.0\n",
      "phie () phiX.shape (200,) target_err (200,) err () num () den ()\n",
      "phie/P 0.000572803383814 phie 0.00114560676763 err/sumGamma 0.136470706571 err 23.1910190636\n",
      "phie () phiX.shape (200,) target_err (200,) err () num () den ()\n",
      "phie/P 0.0356226935967 phie 0.0712453871935 err/sum1-Gamma 0.0887016014429 err 2.66689844062\n",
      "w 0.84281394148 phi_1 [-0.03364863  0.00365736] phi_2 [ 0.23182242 -0.13230175] s2_1 0.136963033673 s2_2 0.107523908808\n",
      "gammaView.shape (1, 200)\n",
      "self.sigma_phi0_inv (2, 2) sigma_phi_inv (2, 2)\n",
      "sum_gammayx, sum_gammaxx SHAPES (2,) (2, 2)\n",
      "sum_gammaxx (2, 2) sum_gammaxy (2,) sigma_mu (2,) sigma_phi_inv (2, 2)\n",
      "phi_1 [-0.04148962  0.01525565]\n",
      "self.sigma_phi0_inv (2, 2) sigma_phi_inv (2, 2)\n",
      "sum_gammayx, sum_gammaxx SHAPES (2,) (2, 2)\n",
      "sum_gammaxx (2, 2) sum_gammaxy (2,) sigma_mu (2,) sigma_phi_inv (2, 2)\n",
      "phi_2 [ 0.23935788 -0.13067303]\n",
      "err_test (200,) <class 'numpy.ndarray'>\n",
      "tsup-err 0.0\n",
      "phie () phiX.shape (200,) target_err (200,) err () num () den ()\n",
      "phie/P 0.000977061537583 phie 0.00195412307517 err/sumGamma 0.131593083244 err 22.0271281521\n",
      "phie () phiX.shape (200,) target_err (200,) err () num () den ()\n",
      "phie/P 0.0371838183648 phie 0.0743676367296 err/sum1-Gamma 0.0931299184728 err 3.03713511358\n",
      "w 0.830334269599 phi_1 [-0.04148962  0.01525565] phi_2 [ 0.23935788 -0.13067303] s2_1 0.132473245049 s2_2 0.109661110774\n",
      "Logl      before     after\n",
      "Term 00:  12.759 <  15.358\n",
      "Term 01: -84.662 < -83.138\n",
      "Term 02:   5.895 <   6.074\n",
      "Term 03: -12.401 > -13.848\n",
      "Term 04: -84.692 > -88.973\n",
      "Term 05:   3.401 =   3.401\n",
      "Term 06:   1.686 >   1.661\n",
      "Term 07:   0.314 <   0.339\n",
      "Term 08:   0.150 <   0.183\n",
      "Term 09:  -0.004 >  -0.007\n",
      "Term 10:   0.392 >   0.372\n",
      "Term 11:  -0.331 >  -0.339\n",
      "Term 12:   0.000 =   0.000\n",
      "Term 13:  -3.178 =  -3.178\n",
      "Term 14:  11.928 <  12.128\n",
      "Term 15:  -7.301 >  -7.549\n",
      "Term 16:   0.000 =   0.000\n",
      "Term 17:  -3.178 =  -3.178\n",
      "Term 18:  13.380 >  13.262\n",
      "Term 19:  -9.300 <  -9.119\n",
      "Total    -155.141   -156.550\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "logl decreased after sigma2 update",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-bf317c0b8bd5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# true model based on the observations (X, Y) we just made\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEM_algo_MM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEM_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model fit (logl %.2f) after %d iterations (%s reached)\"\u001b[0m \u001b[0;34m%\u001b[0m         \u001b[0;34m(\u001b[0m\u001b[0mlogl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-d3ba0fe526a3>\u001b[0m in \u001b[0;36mEM_fit\u001b[0;34m(self, alim, maxit)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mlogl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEM_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m             \u001b[0mlogl2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0madiff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogl2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-21-0aff3cca4835>\u001b[0m in \u001b[0;36mEM_iter\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"w\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"phi_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"phi_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"phi_2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"phi_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s2_1\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sigma2_1\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"s2_2\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sigma2_2\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massert_logl_increased\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sigma2 update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-d3ba0fe526a3>\u001b[0m in \u001b[0;36massert_logl_increased\u001b[0;34m(self, event)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_logl\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mnewlogl\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug_logl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logl decreased after %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_logl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnewlogl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: logl decreased after sigma2 update"
     ]
    }
   ],
   "source": [
    "# generate a model for estimating the parameters of the\n",
    "# true model based on the observations (X, Y) we just made\n",
    "model = EM_algo_MM(hyperp, X, Y)\n",
    "i, logl, r = model.EM_fit()\n",
    "print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "        (logl, i, r))\n",
    "print(\"\")\n",
    "print(\"MAP estimate of true model parameters:\")\n",
    "model.print_p()\n",
    "print(\"\")\n",
    "\n",
    "# if possible, plot samples, true model and estimated model\n",
    "if pdata == 1:\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "#         plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "#         print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "#                 true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "#         print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "#                 model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "\n",
    "    y = true_model.p[\"phi_1\"] * x\n",
    "    color = 'orange'\n",
    "    plt.plot(x, y, color, label=\"true1\")\n",
    "#     plt.fill_between(x, y + 1.96 * sqrt(true_model.p[\"sigma2_1\"]), y - 1.96 * sqrt(true_model.p[\"sigma2_1\"]), alpha=0.1, facecolor=color, interpolate=True)\n",
    "    \n",
    "    y = true_model.p[\"phi_2\"] * x\n",
    "    color = 'green'\n",
    "    plt.plot(x, y, color, label=\"true1\")\n",
    "#     plt.fill_between(x, y + 1.96 * sqrt(true_model.p[\"sigma2_2\"]), y - 1.96 * sqrt(true_model.p[\"sigma2_2\"]), alpha=0.1, facecolor=color, interpolate=True)\n",
    "\n",
    "    # Components\n",
    "    y = model.p[\"phi_1\"] * x\n",
    "    color = 'red'\n",
    "    plt.plot(x, y, color, label=\"component1\")\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(model.p[\"sigma2_1\"]), y - 1.96 * sqrt(model.p[\"sigma2_1\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "    y = model.p[\"phi_2\"] * x\n",
    "    color = 'blue'\n",
    "    plt.plot(x, y, color, label=\"component2\")\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(model.p[\"sigma2_2\"]), y - 1.96 * sqrt(model.p[\"sigma2_2\"]), alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting program\n",
      "\n",
      "Generated 200 training data and 50 validation data from true model:\n",
      "phi    : (0.12, -0.08)\n",
      "sigma2 : 0.147\n",
      "\n",
      "sumxx <class 'numpy.ndarray'> sumxy <class 'numpy.ndarray'> sigma_mu <class 'numpy.ndarray'> sigma_phi_inv <class 'numpy.ndarray'>\n",
      "sumxx (2, 2) sumxy (2,) sigma_mu (2,) sigma_phi_inv (2, 2)\n",
      "phie <class 'numpy.ndarray'> err <class 'numpy.ndarray'> num <class 'numpy.float64'> den <class 'float'>\n",
      "phie (2,) err (200,) num () den is a float\n",
      "sumxx <class 'numpy.ndarray'> sumxy <class 'numpy.ndarray'> sigma_mu <class 'numpy.ndarray'> sigma_phi_inv <class 'numpy.ndarray'>\n",
      "sumxx (2, 2) sumxy (2,) sigma_mu (2,) sigma_phi_inv (2, 2)\n",
      "phie <class 'numpy.ndarray'> err <class 'numpy.ndarray'> num <class 'numpy.float64'> den <class 'float'>\n",
      "phie (2,) err (200,) num () den is a float\n",
      "Model fit (logl -94.43) after 2 iterations (alim reached)\n",
      "\n",
      "MAP estimate of true model parameters:\n",
      "phi    : (0.12, -0.07)\n",
      "sigma2 : 0.152\n",
      "\n",
      "Crossvalidated logl: -16.43\n"
     ]
    }
   ],
   "source": [
    "# Aalto University, School of Science\n",
    "# T-61.5140 Machine Learning: Advanced probabilistic Methods\n",
    "# Author: antti.kangasraasio@aalto.fi, 2016\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "        Executed when program is run.\n",
    "    \"\"\"\n",
    "    print(\"Starting program\")\n",
    "    print(\"\")\n",
    "    test_LM_model()\n",
    "\n",
    "\n",
    "def test_LM_model():\n",
    "    \"\"\"\n",
    "        Example that demonstrates how to call the model.\n",
    "    \"\"\"\n",
    "    # get hyperparameters for model\n",
    "    hyperp = get_hyperp()\n",
    "    # generate 50 training data and 20 validation data locations of dim=1\n",
    "    ndata = 200\n",
    "    ndata_v = 50\n",
    "    pdata = 2\n",
    "    X = generate_X(ndata, pdata)\n",
    "    X_v = generate_X(ndata_v, pdata)\n",
    "    # intialize true model randomly and draw observations from it\n",
    "    true_model = EM_algo_LM(hyperp, ndata=ndata, pdata=pdata)\n",
    "    Y, Z = generate_YZ(X, true_model)\n",
    "    Y_v, Z_v = generate_YZ(X_v, true_model)\n",
    "    print(\"Generated %d training data and %d validation data from true model:\" % \\\n",
    "            (ndata, ndata_v))\n",
    "    true_model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # generate a model for estimating the parameters of the\n",
    "    # true model based on the observations (X, Y) we just made\n",
    "    model = EM_algo_LM(hyperp, X, Y)\n",
    "    i, logl, r = model.EM_fit()\n",
    "    print(\"Model fit (logl %.2f) after %d iterations (%s reached)\" % \\\n",
    "            (logl, i, r))\n",
    "    print(\"\")\n",
    "    print(\"MAP estimate of true model parameters:\")\n",
    "    model.print_p()\n",
    "    print(\"\")\n",
    "\n",
    "    # crossvalidate the estimated model with the validation data\n",
    "    fit_params = model.get_p()\n",
    "    model_v = EM_algo_LM(hyperp, X_v, Y_v)\n",
    "    model_v.set_p(fit_params)\n",
    "    logl, ll = model_v.logl()\n",
    "    print(\"Crossvalidated logl: %.2f\" % (logl))\n",
    "\n",
    "    # if possible, plot samples, true model and estimated model\n",
    "    if pdata != 1:\n",
    "        return\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.scatter(X, Y, s=20, c='black', label=\"Training data\")\n",
    "    plt.scatter(X_v, Y_v, s=20, c='orange', label=\"Validation data\")\n",
    "    x = arange(min(X)-0.1, max(X)+0.1, 0.1)\n",
    "    print_linear_model(x, true_model.get_p()[\"phi\"], \\\n",
    "            true_model.get_p()[\"sigma2\"], 'red', \"True model\")\n",
    "    print_linear_model(x, model.get_p()[\"phi\"], \\\n",
    "            model.get_p()[\"sigma2\"], 'blue', \"Predicted model\")\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlim(min(x), max(x))\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def print_linear_model(x, phi, sigma2, color, label):\n",
    "    \"\"\"\n",
    "        Print linear model mean and 95% confidence interval.\n",
    "    \"\"\"\n",
    "    y = phi * x\n",
    "    plt.plot(x, y, color, label=label)\n",
    "    plt.fill_between(x, y + 1.96 * sqrt(sigma2), y - 1.96 * sqrt(sigma2), \\\n",
    "            alpha=0.25, facecolor=color, interpolate=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = generate_X(10, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.91954461,  0.2400352 ],\n",
       "       [ 2.33650195,  1.81126434],\n",
       "       [-0.08133542,  0.06118832],\n",
       "       [-1.33545433, -0.86719239],\n",
       "       [ 1.66719011, -0.57531528],\n",
       "       [-0.30577782,  0.06323988],\n",
       "       [ 0.99608716, -0.33705257],\n",
       "       [-0.65754579, -0.1670884 ],\n",
       "       [ 1.05556896,  0.3179948 ],\n",
       "       [ 1.17714858,  1.69832692]])"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.05556896,  0.3179948 ])"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[8,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.892341  ,   6.29490444],\n",
       "       [  6.29490444,   7.55600699]])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "true_model = EM_algo_LM(get_hyperp(), ndata=10, pdata=2)\n",
    "Y, Z = generate_YZ(X, true_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-6.63206963, -0.67963419])"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 14.892341  ,   6.29490444],\n",
       "       [  6.29490444,   7.55600699]])"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.T.dot(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  },
  "toc": {
   "toc_cell": false,
   "toc_number_sections": true,
   "toc_threshold": 6,
   "toc_window_display": true
  },
  "toc_position": {
   "left": "1195.078125px",
   "right": "20px",
   "top": "120px",
   "width": "186px"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
